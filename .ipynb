{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepping Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_symbol = 'c'\n",
    "data = pd.read_csv('data/' + stock_symbol + '.csv')\n",
    "\n",
    "# IMPORTANT first 10 entries are not used. Must account for this during usage \n",
    "ma_span = 10\n",
    "data['Date'] = data['Date'].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%d\").date())\n",
    "data['MA-Close'] = data['Close'].rolling(ma_span).mean()\n",
    "data = data[ma_span-1:].reset_index(drop=True)\n",
    "data['MA-Pct-Change'] = (data['MA-Close'].shift(-1) - data['MA-Close']) / data['MA-Close'] * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "NUM_ACTIONS = 2 # 0 -> long, 1 -> close\n",
    "ACTION_LONG = 0\n",
    "ACTION_CLOSE = 1\n",
    "\n",
    "# Adjustable Parameters\n",
    "num_history = 25 # h number of price changes, page 24\n",
    "state_dimension = num_history + 1 # state representation, h number of price changes and current position, page 24\n",
    "comm_percent = 0.2 * 0.01 # 0.2%\n",
    "\n",
    "'''\n",
    "PARAMETERS TO CHANGE\n",
    "'''\n",
    "# start_state_num = state number to start with, correspond to price_index_to_start_reinforcement_learning - num_history\n",
    "start_state_num = 0\n",
    "# number of days to reinforcemently-trade\n",
    "duration = len(data) - num_history - 1\n",
    "\n",
    "assert(start_state_num + duration + num_history <= len(data) - 1)\n",
    "# Important\n",
    "risk_level = -1\n",
    "# Load model?\n",
    "load_model = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot to see bull / bear / catastrophic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig =plt.figure(figsize=(15,5))\n",
    "plt.plot(data['Date'][start_state_num+num_history:start_state_num+num_history+duration], data['MA-Close'][start_state_num+num_history:start_state_num+num_history+duration], figure=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Network & related methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q network to determine the q value of a state\n",
    "class Qnetwork():\n",
    "    def __init__(self, H): # H = neurons in hidden layer\n",
    "        # 1 x (h+1) shape tensor\n",
    "        self.x = tf.placeholder(tf.float32, [1, state_dimension]) \n",
    "        self.W0 = tf.Variable(tf.random_uniform([state_dimension, H], 0, 0.1))\n",
    "        self.b0 = tf.Variable(tf.constant(0.1, shape=[H]))\n",
    "        \n",
    "        # relu activation function on x*W0 + b0\n",
    "        self.y_hidden = tf.nn.relu(tf.matmul(self.x, self.W0) + self.b0) \n",
    "        \n",
    "        # output layer only has NUM_ACTIONS number of neurons\n",
    "        self.W1 = tf.Variable(tf.random_uniform([H, NUM_ACTIONS], 0, 0.1)) \n",
    "        self.b1 = tf.Variable(tf.constant(0.1, shape=[NUM_ACTIONS]))  \n",
    "        \n",
    "         # calculates q value\n",
    "        self.q_values = tf.matmul(self.y_hidden, self.W1) + self.b1\n",
    "        # determine the best action based on q value\n",
    "        self.best_action = tf.argmax(self.q_values, axis=1) \n",
    "        \n",
    "        # the target reward function to \"hopefully\" imitate\n",
    "        self.target = tf.placeholder(tf.float32, [1 ,NUM_ACTIONS]) \n",
    "        \n",
    "        self.loss = tf.reduce_sum(tf.square(self.target - self.q_values))\n",
    "        self.update = tf.train.AdamOptimizer(learning_rate=0.00005).minimize(self.loss)\n",
    "        \n",
    "\n",
    "        \n",
    "# Experience replay, page 27\n",
    "class experience_buffer:\n",
    "    def __init__(self, buffer_size = 10000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "      \n",
    "    # add new experience to memory\n",
    "    def add(self, experience):\n",
    "        if len(self.buffer) + len(experience) >= self.buffer_size:\n",
    "             # remove oldest experience from memory to make room for new \n",
    "            self.buffer[0:(len(experience) + len(self.buffer))-self.buffer_size] = []\n",
    "        self.buffer.extend(experience)\n",
    "        \n",
    "    \n",
    "    def sample(self,size):\n",
    "        return np.reshape(np.array(random.sample(self.buffer,size)),[size,5]) # output size x 5 shaped experience\n",
    "    \n",
    "# Most probably mechanism of separate target network, page 28\n",
    "def updateTargetGraph(tfVars,tau):\n",
    "    total_vars = len(tfVars)\n",
    "    op_holder = []\n",
    "    for idx,var in enumerate(tfVars[0:total_vars//2]):\n",
    "        op_holder.append(tfVars[idx+total_vars//2].assign((var.value()*tau) + ((1-tau)*tfVars[idx+total_vars//2].value())))\n",
    "    return op_holder\n",
    "\n",
    "\n",
    "def updateTarget(op_holder,sess):\n",
    "    for op in op_holder:\n",
    "        sess.run(op)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State related methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the state representation using state number and has_stock flag\n",
    "def processState(state_num, has_stock):\n",
    "    i = state_num\n",
    "    h_price_changes = data['MA-Pct-Change'][i:i+num_history].as_matrix()\n",
    "    state_representation = np.append(h_price_changes, int(has_stock))\n",
    "    return np.reshape(state_representation, (1, state_dimension))\n",
    "\n",
    "# make a copy and reshape? \n",
    "def norm_state(state):\n",
    "    temp = state.copy()\n",
    "    return np.reshape(temp, (1, state_dimension))\n",
    "\n",
    "# reward structure, page 25\n",
    "\n",
    "def get_reward(state_num, state, action):\n",
    "    price_i = state_num + num_history # KIV\n",
    "    if action == ACTION_LONG:\n",
    "        if has_stock(state):\n",
    "            return data['MA-Close'][price_i+1] - data['MA-Close'][price_i]\n",
    "        else:\n",
    "            # no stock, so need to buy -> comission\n",
    "            reward = data['MA-Close'][price_i+1] - data['MA-Close'][price_i] - comm_percent * data['MA-Close'][price_i]\n",
    "            return reward\n",
    "        \n",
    "    else:\n",
    "        if has_stock(state):\n",
    "            reward = - comm_percent * data['MA-Close'][price_i]\n",
    "            return reward\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "'''def get_reward(px, state, action):\n",
    "    temp_comm_percent = comm_percent * 7\n",
    "    px = px + num_history\n",
    "    if action == ACTION_LONG: # long\n",
    "        if has_stock(state):\n",
    "            comm = 0\n",
    "        else:\n",
    "            comm = temp_comm_percent * data['MA-Close'][px]\n",
    "        reward = data['MA-Close'][px + 1] - data['MA-Close'][px] - comm\n",
    "    else: # close\n",
    "        if has_stock(state):\n",
    "            reward = -temp_comm_percent * data['MA-Close'][px]\n",
    "        else:\n",
    "            reward = 0\n",
    "\n",
    "    reward = (reward /data['MA-Close'][px]) * 100\n",
    "\n",
    "    # if reward > 0:\n",
    "    #     reward *= (1 + risk_level)\n",
    "    # else:\n",
    "    #     reward *= (1 - risk_level)\n",
    "\n",
    "    return reward'''\n",
    "\n",
    "def get_action(q_values):\n",
    "    return np.argmax(q_values)\n",
    "\n",
    "def has_stock(state):\n",
    "    # checks state representation if state has stock\n",
    "    return (state[0, state_dimension-1] == 1)\n",
    "\n",
    "\n",
    "def get_next_state(current_state_num, current_state_action):\n",
    "    if current_state_action == ACTION_LONG:\n",
    "        own_stock = True\n",
    "    else:\n",
    "        own_stock = False\n",
    "    return processState(current_state_num+1, own_stock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trading pattern related methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_profit(trading_pattern, start_state_num):\n",
    "    bought = False\n",
    "    profit = 0\n",
    "    for i in range(len(trading_pattern)):\n",
    "        action = trading_pattern[i]\n",
    "        \n",
    "        # first 25 prices are ignored because they are no actions on those days\n",
    "        price_on_day = data['Close'][start_state_num + i + num_history]\n",
    "        \n",
    "        if action == ACTION_LONG:\n",
    "            if not bought:\n",
    "                # Buying of stock\n",
    "                bought = True\n",
    "                profit -= price_on_day\n",
    "                profit -= comm_percent * price_on_day\n",
    "                \n",
    "        else:\n",
    "            if bought:\n",
    "                # Selling of stock\n",
    "                bought=False\n",
    "                profit += price_on_day\n",
    "                profit -= comm_percent * price_on_day\n",
    "    \n",
    "    # If still holding a stock at the end of the trading_pattern\n",
    "    # add current value of stock to profit\n",
    "    if bought == True:\n",
    "        profit += price_on_day        \n",
    "    return profit\n",
    "\n",
    "def get_long_percentage(trading_pattern):\n",
    "    total = len(trading_pattern)\n",
    "    \n",
    "    # because ACTION_CLOSE = 1\n",
    "    long_num = total - np.sum(trading_pattern)\n",
    "    return float(long_num) / total\n",
    "\n",
    "def get_avg_distance_trades(trading_pattern):\n",
    "    prev_action = ACTION_CLOSE\n",
    "    distances = []\n",
    "    start = 0\n",
    "    \n",
    "    for i in range(len(trading_pattern)):\n",
    "        if trading_pattern[i] == ACTION_LONG and prev_action == ACTION_CLOSE:\n",
    "            prev_action = ACTION_LONG\n",
    "            start = i\n",
    "        elif trading_pattern[i] == ACTION_CLOSE and prev_action == ACTION_LONG:\n",
    "            prev_action = ACTION_CLOSE\n",
    "            distances.append(i-start)\n",
    "            \n",
    "    return np.mean(distances)\n",
    "\n",
    "\n",
    "#def plot_trading_pattern()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "update_freq = 10\n",
    "gamma = 0.99\n",
    "startE = 1\n",
    "endE = 0.1\n",
    "annealing_steps = 5000\n",
    "num_episodes = 450\n",
    "pre_train_steps = 150000\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "h_size = 100\n",
    "tau = 0.0005\n",
    "model_dir = \"./models/\" + stock_symbol + \"_\" + str(risk_level) + '/'\n",
    "trading_patterns_dir = \"./trading_patterns/\"\n",
    "\n",
    "#Make a path for our model to be saved in.\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "    \n",
    "if not os.path.exists(trading_patterns_dir):\n",
    "    os.makedirs(trading_patterns_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainQN = Qnetwork(h_size)\n",
    "targetQN = Qnetwork(h_size)\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "trainables = tf.trainable_variables()\n",
    "\n",
    "targetOps = updateTargetGraph(trainables,tau)\n",
    "myBuffer = experience_buffer()\n",
    "\n",
    "#Set the rate of random action decrease.\n",
    "e = startE\n",
    "stepDrop = (startE - endE)/annealing_steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train New Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_model == False:\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        sess.run(init)\n",
    "\n",
    "        updateTarget(targetOps, sess)\n",
    "        total_steps = 0\n",
    "        rAll = 0\n",
    "        profit = 0\n",
    "        reward_arr = []\n",
    "\n",
    "        for i in range(num_episodes):\n",
    "            reward_arr.append(rAll / ((i % 10) + 1))\n",
    "\n",
    "            # Show average results per 10 episodes\n",
    "            if i % 10 == 0:\n",
    "                print(\"Episode %d, Reward %f, Profit %f\" % (i, rAll / 10, profit / 10))\n",
    "                rAll = 0\n",
    "                profit = 0\n",
    "\n",
    "            episodeBuffer = experience_buffer()\n",
    "\n",
    "\n",
    "            current_state = processState(start_state_num, False)\n",
    "            trading_pattern = []\n",
    "\n",
    "            for current_state_num in range(start_state_num, start_state_num+duration):\n",
    "\n",
    "                # if choose exploration (exploration vs exploitation) or less than pre_train_steps (generate experience)\n",
    "                if np.random.rand(1) < e or total_steps < pre_train_steps:\n",
    "                    action = np.random.randint(0, 2)\n",
    "                else:\n",
    "                    q_values = sess.run(mainQN.q_values, feed_dict={mainQN.x: norm_state(current_state)})\n",
    "                    action = get_action(q_values)\n",
    "\n",
    "                trading_pattern.append(action)\n",
    "                r = get_reward(current_state_num, current_state, action)\n",
    "                rAll += r\n",
    "\n",
    "                next_state = get_next_state(current_state_num, action)\n",
    "                next_state_num = current_state_num + 1\n",
    "\n",
    "                # investigate this\n",
    "                episodeBuffer.add(np.reshape(np.array([current_state, action, r, next_state, next_state_num == duration + start_state_num]), [1, 5]))\n",
    "\n",
    "                # if on the last day to reinforcemently-trade in this episode, calculate profit from this trading pattern\n",
    "                if next_state_num == start_state_num + duration:\n",
    "                    profit += calculate_profit(trading_pattern, start_state_num)\n",
    "                    print(get_long_percentage(trading_pattern))\n",
    "\n",
    "\n",
    "                total_steps += 1\n",
    "                if total_steps > pre_train_steps:\n",
    "                    if e > endE:\n",
    "                        e -= stepDrop\n",
    "\n",
    "                    if total_steps % update_freq == 0:\n",
    "                        train_batch = myBuffer.sample(batch_size)\n",
    "\n",
    "                        for tb in range(batch_size):\n",
    "                            Q1 = sess.run(mainQN.q_values, feed_dict={mainQN.x: norm_state(train_batch[tb, 0])})\n",
    "                            Q2 = sess.run(targetQN.q_values, feed_dict={targetQN.x: norm_state(train_batch[tb,3])})\n",
    "                            end_multiplier = 1 - train_batch[tb, 4]\n",
    "                            target = train_batch[tb, 2] + end_multiplier * gamma * np.max(Q2)\n",
    "                            temporal_difference = target - Q1[0, train_batch[tb, 1]]\n",
    "                            if temporal_difference > 0:\n",
    "                                target = target + temporal_difference * risk_level\n",
    "                            else:\n",
    "                                target = target - temporal_difference * risk_level\n",
    "                            Q1[0, train_batch[tb, 1]] = target\n",
    "\n",
    "                            sess.run(mainQN.update, feed_dict={mainQN.target: Q1, mainQN.x: norm_state(train_batch[tb, 0])})\n",
    "                        updateTarget(targetOps, sess)\n",
    "                current_state = next_state\n",
    "            myBuffer.add(episodeBuffer.buffer)\n",
    "\n",
    "            if i == num_episodes - 1 or (i % 50 == 0 and i > 200):\n",
    "                saver.save(sess,model_dir+'model.cptk')\n",
    "                print(\"Saved model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trading_pattern(trading_pattern, plotter):\n",
    "    for i in range(len(trading_pattern)):\n",
    "        if trading_pattern[i] == ACTION_LONG:\n",
    "            trading_pattern[i] = 1\n",
    "        else:\n",
    "            trading_pattern[i] = 0\n",
    "    plotter.plot(range(start_state_num+num_history, start_state_num+duration+num_history), trading_pattern)\n",
    "    plotter.fill_between(range(start_state_num+num_history, start_state_num+duration+num_history), trading_pattern)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_model:\n",
    "    with tf.Session() as sess:\n",
    "        print('Loading model')\n",
    "        ckpt = tf.train.get_checkpoint_state(model_dir)\n",
    "        saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "        \n",
    "        rAll = 0\n",
    "        trading_pattern = []\n",
    "        trading_buy_x = []\n",
    "        trading_buy_y = []\n",
    "        trading_sell_x = []\n",
    "        trading_sell_y = []\n",
    "        current_state = processState(start_state_num, False)\n",
    "        prev_action = ACTION_CLOSE\n",
    "\n",
    "        for current_state_num in range(start_state_num, start_state_num + duration):\n",
    "            qv = sess.run(mainQN.q_values, feed_dict={mainQN.x: norm_state(current_state)})\n",
    "            action = get_action(qv)\n",
    "            r = get_reward(current_state_num, current_state, action)\n",
    "            rAll += r\n",
    "            trading_pattern.append(action)\n",
    "            \n",
    "            if action == ACTION_LONG:\n",
    "                if prev_action == ACTION_CLOSE:\n",
    "                    # trading_buy_x = Day \n",
    "                    trading_buy_x.append(current_state_num + num_history)\n",
    "                    trading_buy_y.append(data['MA-Close'][current_state_num + num_history])\n",
    "            else:\n",
    "                if prev_action == ACTION_LONG:\n",
    "                    trading_sell_x.append(current_state_num + num_history)\n",
    "                    trading_sell_y.append(data['MA-Close'][current_state_num + num_history])\n",
    "            prev_action = action\n",
    "            current_state = get_next_state(current_state_num, action)\n",
    "            \n",
    "            \n",
    "        data['trading_action'] = np.nan\n",
    "        data['trading_action'][start_state_num + num_history:start_state_num + duration + num_history] = trading_pattern\n",
    "        data.to_csv(trading_patterns_dir + 'c_'+str(risk_level)+'_'+str(start_state_num+num_history)+'_to_' + str(start_state_num+duration+num_history) + '.csv', index=False)\n",
    "        \n",
    "        print(trading_pattern)\n",
    "        print(trading_buy_x)\n",
    "        print(trading_sell_x)\n",
    "        print(rAll)\n",
    "        _, axarr = plt.subplots(2, sharex=True)\n",
    "        print(calculate_profit(trading_pattern, start_state_num))\n",
    "        print(\"Long percentage: %f\" % (get_long_percentage(trading_pattern)))\n",
    "        print(\"Avg days: %f\" % (get_avg_distance_trades(trading_pattern)))\n",
    "        axarr[0].plot(range(start_state_num, start_state_num+duration+num_history), data['MA-Close'][start_state_num:start_state_num+duration+num_history], 'g')\n",
    "        axarr[0].plot(trading_buy_x, trading_buy_y, 'yo', label='Long')\n",
    "        axarr[0].plot(trading_sell_x, trading_sell_y, 'rs', label='Close')\n",
    "        axarr[0].legend(loc='upper left')\n",
    "        axarr[0].set_ylabel('Stock price')\n",
    "        axarr[1].set_xlabel('Days')\n",
    "        axarr[1].set_ylabel('Action value')\n",
    "        axarr[1].set_yticks([0, 1])\n",
    "        plot_trading_pattern(trading_pattern, axarr[1])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
