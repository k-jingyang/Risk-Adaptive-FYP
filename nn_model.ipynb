{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.python import pywrap_tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "PARAMETERS TO CHANGE\n",
    "'''\n",
    "stock_symbol = 'c'\n",
    "train_model = True\n",
    "test_model = True\n",
    "''''''\n",
    "\n",
    "training_data_dir = './training_data/'\n",
    "y_columns = ['ppo', 'rsi']\n",
    "#assert(train_model ^ test_model)\n",
    "model_dir = \"./regularizer_models/\" + stock_symbol + '/'\n",
    "\n",
    "if train_model:\n",
    "    training_data = pd.read_csv(training_data_dir + stock_symbol + '_training_data.csv')\n",
    "    unneeded_columns = ['window']\n",
    "\n",
    "    x_columns = list(training_data.columns)\n",
    "    for to_remove in (y_columns + unneeded_columns):\n",
    "        x_columns.remove(to_remove)\n",
    "\n",
    "    train_x = training_data[x_columns].values\n",
    "    train_y = training_data[y_columns].values\n",
    "\n",
    "    model_dir = \"./regularizer_models/\" + stock_symbol + '/'\n",
    "\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "        \n",
    "if test_model:\n",
    "    test_data = pd.read_csv(training_data_dir + stock_symbol + '_test_data.csv')\n",
    "    \n",
    "    unneeded_columns = ['Date_to_use_on']\n",
    "    x_columns = list(test_data.columns)\n",
    "\n",
    "    for to_remove in (unneeded_columns):\n",
    "        x_columns.remove(to_remove)\n",
    "\n",
    "    \n",
    "    test_x = test_data[x_columns].values\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.1\n",
    "num_epochs = 50000\n",
    "batch_size = 128\n",
    "display_epoch = 100\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of neurons\n",
    "n_hidden_2 = 256 # 2nd layer number of neurons\n",
    "num_input = len(x_columns) # Number of features\n",
    "num_output = len(y_columns) # PPO, RSI parameters\n",
    "\n",
    "\n",
    "def ANN(X):\n",
    "    # Store layers weight & bias\n",
    "    weights = {\n",
    "        'h1': tf.Variable(tf.random_normal([num_input, n_hidden_1])),\n",
    "        'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden_2, num_output]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "        'out': tf.Variable(tf.random_normal([num_output]))\n",
    "    }\n",
    "\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_1 = tf.add(tf.matmul(X, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)    \n",
    "    \n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    \n",
    "    \n",
    "    # Output fully connected layer with a neuron for each output\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    out = tf.square(out_layer)\n",
    "    \n",
    "    return out\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_output])\n",
    "\n",
    "output = ANN(X)\n",
    "\n",
    "# Loss & Optimizer\n",
    "loss_op = tf.losses.mean_squared_error(Y, output, weights=1)\n",
    "train_op = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss_op)\n",
    "# Accuracy\n",
    "accuracy =  tf.abs(tf.reduce_mean(tf.round(output) - Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss= 527105523712.0000, Training Error= 569576.438\n",
      "Epoch 1, Loss= 823674816.0000, Training Error= 26432.018\n",
      "Epoch 100, Loss= 26882.8711, Training Error= 83.490\n",
      "Epoch 200, Loss= 14317.2344, Training Error= 54.336\n",
      "Epoch 300, Loss= 8758.9229, Training Error= 42.128\n",
      "Epoch 400, Loss= 6961.5430, Training Error= 37.131\n",
      "Epoch 500, Loss= 5600.5503, Training Error= 33.043\n",
      "Epoch 600, Loss= 4643.3652, Training Error= 29.864\n",
      "Epoch 700, Loss= 3925.2563, Training Error= 27.202\n",
      "Epoch 800, Loss= 3364.1450, Training Error= 24.912\n",
      "Epoch 900, Loss= 2917.4702, Training Error= 22.930\n",
      "Epoch 1000, Loss= 2564.4294, Training Error= 21.261\n",
      "Epoch 1100, Loss= 2233.5203, Training Error= 19.770\n",
      "Epoch 1200, Loss= 1952.1266, Training Error= 18.416\n",
      "Epoch 1300, Loss= 1746.6244, Training Error= 17.327\n",
      "Epoch 1400, Loss= 1579.1211, Training Error= 16.420\n",
      "Epoch 1500, Loss= 1435.3488, Training Error= 15.634\n",
      "Epoch 1600, Loss= 1317.8368, Training Error= 14.956\n",
      "Epoch 1700, Loss= 1221.1279, Training Error= 14.386\n",
      "Epoch 1800, Loss= 1140.5483, Training Error= 13.882\n",
      "Epoch 1900, Loss= 1072.0305, Training Error= 13.435\n",
      "Epoch 2000, Loss= 1013.9929, Training Error= 13.050\n",
      "Epoch 2100, Loss= 961.8818, Training Error= 12.663\n",
      "Epoch 2200, Loss= 914.3694, Training Error= 12.328\n",
      "Epoch 2300, Loss= 872.9246, Training Error= 12.014\n",
      "Epoch 2400, Loss= 835.8441, Training Error= 11.725\n",
      "Epoch 2500, Loss= 802.3063, Training Error= 11.472\n",
      "Epoch 2600, Loss= 771.9852, Training Error= 11.236\n",
      "Epoch 2700, Loss= 743.4402, Training Error= 11.009\n",
      "Epoch 2800, Loss= 717.0646, Training Error= 10.808\n",
      "Epoch 2900, Loss= 692.8956, Training Error= 10.620\n",
      "Epoch 3000, Loss= 653.7404, Training Error= 10.273\n",
      "Epoch 3100, Loss= 621.7819, Training Error= 9.964\n",
      "Epoch 3200, Loss= 596.0193, Training Error= 9.738\n",
      "Epoch 3300, Loss= 569.9420, Training Error= 9.502\n",
      "Epoch 3400, Loss= 547.1166, Training Error= 9.295\n",
      "Epoch 3500, Loss= 523.7360, Training Error= 9.055\n",
      "Epoch 3600, Loss= 504.9414, Training Error= 8.868\n",
      "Epoch 3700, Loss= 487.5533, Training Error= 8.715\n",
      "Epoch 3800, Loss= 471.0470, Training Error= 8.557\n",
      "Epoch 3900, Loss= 454.8998, Training Error= 8.392\n",
      "Epoch 4000, Loss= 439.8454, Training Error= 8.222\n",
      "Epoch 4100, Loss= 425.0339, Training Error= 8.086\n",
      "Epoch 4200, Loss= 411.1376, Training Error= 7.947\n",
      "Epoch 4300, Loss= 397.2761, Training Error= 7.811\n",
      "Epoch 4400, Loss= 384.2520, Training Error= 7.675\n",
      "Epoch 4500, Loss= 372.2238, Training Error= 7.537\n",
      "Epoch 4600, Loss= 360.6459, Training Error= 7.427\n",
      "Epoch 4700, Loss= 348.6162, Training Error= 7.285\n",
      "Epoch 4800, Loss= 337.6334, Training Error= 7.163\n",
      "Epoch 4900, Loss= 326.2329, Training Error= 7.036\n",
      "Epoch 5000, Loss= 315.1191, Training Error= 6.888\n",
      "Epoch 5100, Loss= 304.0857, Training Error= 6.757\n",
      "Epoch 5200, Loss= 292.4832, Training Error= 6.623\n",
      "Epoch 5300, Loss= 281.0128, Training Error= 6.441\n",
      "Epoch 5400, Loss= 269.7552, Training Error= 6.291\n",
      "Epoch 5500, Loss= 258.4009, Training Error= 6.141\n",
      "Epoch 5600, Loss= 248.6523, Training Error= 6.007\n",
      "Epoch 5700, Loss= 239.5520, Training Error= 5.865\n",
      "Epoch 5800, Loss= 230.8908, Training Error= 5.741\n",
      "Epoch 5900, Loss= 222.8846, Training Error= 5.612\n",
      "Epoch 6000, Loss= 215.5998, Training Error= 5.489\n",
      "Epoch 6100, Loss= 208.4666, Training Error= 5.382\n",
      "Epoch 6200, Loss= 200.8699, Training Error= 5.281\n",
      "Epoch 6300, Loss= 193.9740, Training Error= 5.159\n",
      "Epoch 6400, Loss= 187.6285, Training Error= 5.051\n",
      "Epoch 6500, Loss= 181.3141, Training Error= 4.952\n",
      "Epoch 6600, Loss= 175.3327, Training Error= 4.844\n",
      "Epoch 6700, Loss= 169.7676, Training Error= 4.749\n",
      "Epoch 6800, Loss= 164.4300, Training Error= 4.641\n",
      "Epoch 6900, Loss= 159.4719, Training Error= 4.544\n",
      "Epoch 7000, Loss= 154.7891, Training Error= 4.458\n",
      "Epoch 7100, Loss= 150.3219, Training Error= 4.368\n",
      "Epoch 7200, Loss= 146.0410, Training Error= 4.278\n",
      "Epoch 7300, Loss= 141.9298, Training Error= 4.195\n",
      "Epoch 7400, Loss= 138.0038, Training Error= 4.108\n",
      "Epoch 7500, Loss= 134.1460, Training Error= 4.040\n",
      "Epoch 7600, Loss= 130.5039, Training Error= 3.952\n",
      "Epoch 7700, Loss= 126.7026, Training Error= 3.850\n",
      "Epoch 7800, Loss= 122.8463, Training Error= 3.770\n",
      "Epoch 7900, Loss= 119.4727, Training Error= 3.683\n",
      "Epoch 8000, Loss= 116.2346, Training Error= 3.605\n",
      "Epoch 8100, Loss= 113.0830, Training Error= 3.530\n",
      "Epoch 8200, Loss= 109.9491, Training Error= 3.454\n",
      "Epoch 8300, Loss= 106.9601, Training Error= 3.372\n",
      "Epoch 8400, Loss= 104.0909, Training Error= 3.288\n",
      "Epoch 8500, Loss= 101.2878, Training Error= 3.218\n",
      "Epoch 8600, Loss= 97.2400, Training Error= 3.107\n",
      "Epoch 8700, Loss= 94.2290, Training Error= 3.036\n",
      "Epoch 8800, Loss= 91.5473, Training Error= 2.970\n",
      "Epoch 8900, Loss= 88.9140, Training Error= 2.900\n",
      "Epoch 9000, Loss= 86.4231, Training Error= 2.838\n",
      "Epoch 9100, Loss= 83.9691, Training Error= 2.748\n",
      "Epoch 9200, Loss= 81.5679, Training Error= 2.659\n",
      "Epoch 9300, Loss= 79.0553, Training Error= 2.594\n",
      "Epoch 9400, Loss= 76.7598, Training Error= 2.505\n",
      "Epoch 9500, Loss= 74.7150, Training Error= 2.442\n",
      "Epoch 9600, Loss= 72.7865, Training Error= 2.369\n",
      "Epoch 9700, Loss= 70.7355, Training Error= 2.306\n",
      "Epoch 9800, Loss= 68.8776, Training Error= 2.234\n",
      "Epoch 9900, Loss= 67.0632, Training Error= 2.167\n",
      "Epoch 10000, Loss= 65.3139, Training Error= 2.100\n",
      "Epoch 10100, Loss= 63.6720, Training Error= 2.033\n",
      "Epoch 10200, Loss= 62.0808, Training Error= 1.973\n",
      "Epoch 10300, Loss= 60.5632, Training Error= 1.898\n",
      "Epoch 10400, Loss= 59.1183, Training Error= 1.831\n",
      "Epoch 10500, Loss= 57.7395, Training Error= 1.766\n",
      "Epoch 10600, Loss= 56.4089, Training Error= 1.703\n",
      "Epoch 10700, Loss= 55.1387, Training Error= 1.641\n",
      "Epoch 10800, Loss= 53.9142, Training Error= 1.584\n",
      "Epoch 10900, Loss= 52.7378, Training Error= 1.526\n",
      "Epoch 11000, Loss= 50.9482, Training Error= 1.443\n",
      "Epoch 11100, Loss= 49.8716, Training Error= 1.387\n",
      "Epoch 11200, Loss= 48.8198, Training Error= 1.336\n",
      "Epoch 11300, Loss= 47.8206, Training Error= 1.286\n",
      "Epoch 11400, Loss= 46.8620, Training Error= 1.229\n",
      "Epoch 11500, Loss= 45.9343, Training Error= 1.172\n",
      "Epoch 11600, Loss= 45.0326, Training Error= 1.123\n",
      "Epoch 11700, Loss= 44.1664, Training Error= 1.079\n",
      "Epoch 11800, Loss= 43.3278, Training Error= 1.023\n",
      "Epoch 11900, Loss= 42.5147, Training Error= 0.974\n",
      "Epoch 12000, Loss= 41.7288, Training Error= 0.927\n",
      "Epoch 12100, Loss= 40.9667, Training Error= 0.885\n",
      "Epoch 12200, Loss= 40.1781, Training Error= 0.835\n",
      "Epoch 12300, Loss= 39.4619, Training Error= 0.793\n",
      "Epoch 12400, Loss= 38.7712, Training Error= 0.754\n",
      "Epoch 12500, Loss= 38.1008, Training Error= 0.707\n",
      "Epoch 12600, Loss= 37.4482, Training Error= 0.665\n",
      "Epoch 12700, Loss= 36.8158, Training Error= 0.621\n",
      "Epoch 12800, Loss= 36.2034, Training Error= 0.576\n",
      "Epoch 12900, Loss= 35.5882, Training Error= 0.530\n",
      "Epoch 13000, Loss= 34.9233, Training Error= 0.485\n",
      "Epoch 13100, Loss= 34.3253, Training Error= 0.445\n",
      "Epoch 13200, Loss= 33.7464, Training Error= 0.406\n",
      "Epoch 13300, Loss= 33.1922, Training Error= 0.365\n",
      "Epoch 13400, Loss= 32.6712, Training Error= 0.316\n",
      "Epoch 13500, Loss= 32.1389, Training Error= 0.289\n",
      "Epoch 13600, Loss= 31.6349, Training Error= 0.243\n",
      "Epoch 13700, Loss= 31.1590, Training Error= 0.210\n",
      "Epoch 13800, Loss= 30.6804, Training Error= 0.165\n",
      "Epoch 13900, Loss= 30.2276, Training Error= 0.133\n",
      "Epoch 14000, Loss= 29.7981, Training Error= 0.103\n",
      "Epoch 14100, Loss= 29.3706, Training Error= 0.068\n",
      "Epoch 14200, Loss= 28.9778, Training Error= 0.031\n",
      "Epoch 14300, Loss= 28.5516, Training Error= 0.001\n",
      "Epoch 14400, Loss= 28.1920, Training Error= 0.028\n",
      "Epoch 14500, Loss= 27.7663, Training Error= 0.059\n",
      "Epoch 14600, Loss= 27.4075, Training Error= 0.091\n",
      "Epoch 14700, Loss= 27.0391, Training Error= 0.117\n",
      "Epoch 14800, Loss= 26.6003, Training Error= 0.163\n",
      "Epoch 14900, Loss= 26.2523, Training Error= 0.195\n",
      "Epoch 15000, Loss= 25.9324, Training Error= 0.218\n",
      "Epoch 15100, Loss= 25.6153, Training Error= 0.243\n",
      "Epoch 15200, Loss= 25.2959, Training Error= 0.271\n",
      "Epoch 15300, Loss= 24.9989, Training Error= 0.307\n",
      "Epoch 15400, Loss= 24.7211, Training Error= 0.320\n",
      "Epoch 15500, Loss= 24.4400, Training Error= 0.348\n",
      "Epoch 15600, Loss= 24.1885, Training Error= 0.365\n",
      "Epoch 15700, Loss= 23.9508, Training Error= 0.383\n",
      "Epoch 15800, Loss= 23.6345, Training Error= 0.416\n",
      "Epoch 15900, Loss= 23.3703, Training Error= 0.435\n",
      "Epoch 16000, Loss= 23.1165, Training Error= 0.457\n",
      "Epoch 16100, Loss= 22.8653, Training Error= 0.469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16200, Loss= 22.6070, Training Error= 0.490\n",
      "Epoch 16300, Loss= 22.3487, Training Error= 0.498\n",
      "Epoch 16400, Loss= 22.0901, Training Error= 0.525\n",
      "Epoch 16500, Loss= 21.8228, Training Error= 0.533\n",
      "Epoch 16600, Loss= 21.6012, Training Error= 0.544\n",
      "Epoch 16700, Loss= 21.3643, Training Error= 0.564\n",
      "Epoch 16800, Loss= 21.1500, Training Error= 0.577\n",
      "Epoch 16900, Loss= 20.9501, Training Error= 0.588\n",
      "Epoch 17000, Loss= 20.7527, Training Error= 0.596\n",
      "Epoch 17100, Loss= 20.5618, Training Error= 0.612\n",
      "Epoch 17200, Loss= 20.3622, Training Error= 0.619\n",
      "Epoch 17300, Loss= 20.1698, Training Error= 0.629\n",
      "Epoch 17400, Loss= 19.9907, Training Error= 0.644\n",
      "Epoch 17500, Loss= 19.8135, Training Error= 0.651\n",
      "Epoch 17600, Loss= 19.6390, Training Error= 0.656\n",
      "Epoch 17700, Loss= 19.4619, Training Error= 0.661\n",
      "Epoch 17800, Loss= 19.2901, Training Error= 0.666\n",
      "Epoch 17900, Loss= 19.1046, Training Error= 0.677\n",
      "Epoch 18000, Loss= 18.9254, Training Error= 0.686\n",
      "Epoch 18100, Loss= 18.7472, Training Error= 0.688\n",
      "Epoch 18200, Loss= 18.5775, Training Error= 0.686\n",
      "Epoch 18300, Loss= 18.7060, Training Error= 0.662\n",
      "Epoch 18400, Loss= 18.2058, Training Error= 0.703\n",
      "Epoch 18500, Loss= 18.0330, Training Error= 0.706\n",
      "Epoch 18600, Loss= 17.8666, Training Error= 0.710\n",
      "Epoch 18700, Loss= 17.7241, Training Error= 0.716\n",
      "Epoch 18800, Loss= 17.5229, Training Error= 0.713\n",
      "Epoch 18900, Loss= 17.3923, Training Error= 0.717\n",
      "Epoch 19000, Loss= 17.2153, Training Error= 0.718\n",
      "Epoch 19100, Loss= 17.1738, Training Error= 0.729\n",
      "Epoch 19200, Loss= 16.8397, Training Error= 0.715\n",
      "Epoch 19300, Loss= 16.7192, Training Error= 0.708\n",
      "Epoch 19400, Loss= 16.4802, Training Error= 0.712\n",
      "Epoch 19500, Loss= 16.3158, Training Error= 0.705\n",
      "Epoch 19600, Loss= 16.1230, Training Error= 0.705\n",
      "Epoch 19700, Loss= 15.9457, Training Error= 0.705\n",
      "Epoch 19800, Loss= 15.7602, Training Error= 0.707\n",
      "Epoch 19900, Loss= 15.5714, Training Error= 0.708\n",
      "Epoch 20000, Loss= 15.4197, Training Error= 0.693\n",
      "Epoch 20100, Loss= 15.2084, Training Error= 0.687\n",
      "Epoch 20200, Loss= 15.0081, Training Error= 0.686\n",
      "Epoch 20300, Loss= 14.9855, Training Error= 0.670\n",
      "Epoch 20400, Loss= 14.5907, Training Error= 0.671\n",
      "Epoch 20500, Loss= 14.4612, Training Error= 0.673\n",
      "Epoch 20600, Loss= 14.1151, Training Error= 0.660\n",
      "Epoch 20700, Loss= 13.9729, Training Error= 0.639\n",
      "Epoch 20800, Loss= 13.7503, Training Error= 0.649\n",
      "Epoch 20900, Loss= 13.4438, Training Error= 0.629\n",
      "Epoch 21000, Loss= 13.1414, Training Error= 0.611\n",
      "Epoch 21100, Loss= 12.8995, Training Error= 0.606\n",
      "Epoch 21200, Loss= 12.6468, Training Error= 0.588\n",
      "Epoch 21300, Loss= 12.6263, Training Error= 0.603\n",
      "Epoch 21400, Loss= 12.1368, Training Error= 0.565\n",
      "Epoch 21500, Loss= 11.8839, Training Error= 0.546\n",
      "Epoch 21600, Loss= 11.5960, Training Error= 0.536\n",
      "Epoch 21700, Loss= 11.3328, Training Error= 0.534\n",
      "Epoch 21800, Loss= 11.0634, Training Error= 0.519\n",
      "Epoch 21900, Loss= 10.7544, Training Error= 0.507\n",
      "Epoch 22000, Loss= 10.5595, Training Error= 0.461\n",
      "Epoch 22100, Loss= 10.2492, Training Error= 0.465\n",
      "Epoch 22200, Loss= 9.9974, Training Error= 0.458\n",
      "Epoch 22300, Loss= 9.8585, Training Error= 0.427\n",
      "Epoch 22400, Loss= 18173.1348, Training Error= 76.610\n",
      "Epoch 22500, Loss= 953.3013, Training Error= 13.518\n",
      "Epoch 22600, Loss= 633.3654, Training Error= 9.838\n",
      "Epoch 22700, Loss= 452.8095, Training Error= 8.078\n",
      "Epoch 22800, Loss= 347.7869, Training Error= 6.888\n",
      "Epoch 22900, Loss= 288.2668, Training Error= 6.095\n",
      "Epoch 23000, Loss= 248.8071, Training Error= 5.522\n",
      "Epoch 23100, Loss= 219.3212, Training Error= 5.007\n",
      "Epoch 23200, Loss= 195.7852, Training Error= 4.563\n",
      "Epoch 23300, Loss= 175.4328, Training Error= 4.195\n",
      "Epoch 23400, Loss= 158.3988, Training Error= 3.848\n",
      "Epoch 23500, Loss= 143.6895, Training Error= 3.517\n",
      "Epoch 23600, Loss= 130.7226, Training Error= 3.213\n",
      "Epoch 23700, Loss= 119.4544, Training Error= 2.948\n",
      "Epoch 23800, Loss= 109.6108, Training Error= 2.685\n",
      "Epoch 23900, Loss= 101.0522, Training Error= 2.462\n",
      "Epoch 24000, Loss= 93.5729, Training Error= 2.254\n",
      "Epoch 24100, Loss= 86.9016, Training Error= 2.009\n",
      "Epoch 24200, Loss= 81.0774, Training Error= 1.842\n",
      "Epoch 24300, Loss= 75.9956, Training Error= 1.679\n",
      "Epoch 24400, Loss= 71.6300, Training Error= 1.559\n",
      "Epoch 24500, Loss= 67.5536, Training Error= 1.381\n",
      "Epoch 24600, Loss= 64.1178, Training Error= 1.233\n",
      "Epoch 24700, Loss= 60.9535, Training Error= 1.099\n",
      "Epoch 24800, Loss= 58.1066, Training Error= 1.008\n",
      "Epoch 24900, Loss= 55.4144, Training Error= 0.874\n",
      "Epoch 25000, Loss= 53.0991, Training Error= 0.808\n",
      "Epoch 25100, Loss= 50.9197, Training Error= 0.673\n",
      "Epoch 25200, Loss= 48.9895, Training Error= 0.584\n",
      "Epoch 25300, Loss= 47.1530, Training Error= 0.515\n",
      "Epoch 25400, Loss= 45.4158, Training Error= 0.437\n",
      "Epoch 25500, Loss= 43.7008, Training Error= 0.348\n",
      "Epoch 25600, Loss= 42.2262, Training Error= 0.231\n",
      "Epoch 25700, Loss= 40.8224, Training Error= 0.176\n",
      "Epoch 25800, Loss= 39.7051, Training Error= 0.164\n",
      "Epoch 25900, Loss= 38.5551, Training Error= 0.041\n",
      "Epoch 26000, Loss= 37.3410, Training Error= 0.014\n",
      "Epoch 26100, Loss= 36.3129, Training Error= 0.075\n",
      "Epoch 26200, Loss= 35.4089, Training Error= 0.131\n",
      "Epoch 26300, Loss= 34.5752, Training Error= 0.180\n",
      "Epoch 26400, Loss= 33.7399, Training Error= 0.232\n",
      "Epoch 26500, Loss= 32.9746, Training Error= 0.280\n",
      "Epoch 26600, Loss= 32.1866, Training Error= 0.334\n",
      "Epoch 26700, Loss= 31.4472, Training Error= 0.385\n",
      "Epoch 26800, Loss= 30.7431, Training Error= 0.424\n",
      "Epoch 26900, Loss= 30.0677, Training Error= 0.475\n",
      "Epoch 27000, Loss= 163.7274, Training Error= 2.858\n",
      "Epoch 27100, Loss= 88.8730, Training Error= 1.816\n",
      "Epoch 27200, Loss= 70.5502, Training Error= 1.376\n",
      "Epoch 27300, Loss= 59.8555, Training Error= 1.058\n",
      "Epoch 27400, Loss= 52.5176, Training Error= 0.821\n",
      "Epoch 27500, Loss= 47.1124, Training Error= 0.627\n",
      "Epoch 27600, Loss= 43.1663, Training Error= 0.463\n",
      "Epoch 27700, Loss= 40.0859, Training Error= 0.334\n",
      "Epoch 27800, Loss= 37.6118, Training Error= 0.217\n",
      "Epoch 27900, Loss= 35.5652, Training Error= 0.113\n",
      "Epoch 28000, Loss= 33.8872, Training Error= 0.022\n",
      "Epoch 28100, Loss= 32.4894, Training Error= 0.061\n",
      "Epoch 28200, Loss= 31.3221, Training Error= 0.143\n",
      "Epoch 28300, Loss= 30.3409, Training Error= 0.206\n",
      "Epoch 28400, Loss= 29.5390, Training Error= 0.263\n",
      "Epoch 28500, Loss= 28.8467, Training Error= 0.319\n",
      "Epoch 28600, Loss= 28.2361, Training Error= 0.376\n",
      "Epoch 28700, Loss= 27.7018, Training Error= 0.411\n",
      "Epoch 28800, Loss= 27.2003, Training Error= 0.448\n",
      "Epoch 28900, Loss= 26.7631, Training Error= 0.488\n",
      "Epoch 29000, Loss= 26.3613, Training Error= 0.520\n",
      "Epoch 29100, Loss= 26.0003, Training Error= 0.555\n",
      "Epoch 29200, Loss= 25.6741, Training Error= 0.584\n",
      "Epoch 29300, Loss= 25.3622, Training Error= 0.606\n",
      "Epoch 29400, Loss= 25.0681, Training Error= 0.636\n",
      "Epoch 29500, Loss= 24.7916, Training Error= 0.664\n",
      "Epoch 29600, Loss= 24.5380, Training Error= 0.680\n",
      "Epoch 29700, Loss= 24.3048, Training Error= 0.694\n",
      "Epoch 29800, Loss= 24.0844, Training Error= 0.716\n",
      "Epoch 29900, Loss= 23.8755, Training Error= 0.734\n",
      "Epoch 30000, Loss= 23.6546, Training Error= 0.742\n",
      "Epoch 30100, Loss= 23.4492, Training Error= 0.752\n",
      "Epoch 30200, Loss= 23.2568, Training Error= 0.758\n",
      "Epoch 30300, Loss= 23.0728, Training Error= 0.765\n",
      "Epoch 30400, Loss= 22.8898, Training Error= 0.775\n",
      "Epoch 30500, Loss= 22.7098, Training Error= 0.782\n",
      "Epoch 30600, Loss= 22.5372, Training Error= 0.786\n",
      "Epoch 30700, Loss= 22.3680, Training Error= 0.783\n",
      "Epoch 30800, Loss= 22.2007, Training Error= 0.779\n",
      "Epoch 30900, Loss= 22.0397, Training Error= 0.779\n",
      "Epoch 31000, Loss= 21.8828, Training Error= 0.775\n",
      "Epoch 31100, Loss= 21.7294, Training Error= 0.768\n",
      "Epoch 31200, Loss= 21.5794, Training Error= 0.762\n",
      "Epoch 31300, Loss= 21.4219, Training Error= 0.754\n",
      "Epoch 31400, Loss= 21.2687, Training Error= 0.745\n",
      "Epoch 31500, Loss= 21.1169, Training Error= 0.736\n",
      "Epoch 31600, Loss= 20.9691, Training Error= 0.723\n",
      "Epoch 31700, Loss= 20.8175, Training Error= 0.701\n",
      "Epoch 31800, Loss= 20.6667, Training Error= 0.686\n",
      "Epoch 31900, Loss= 20.5193, Training Error= 0.672\n",
      "Epoch 32000, Loss= 20.3760, Training Error= 0.658\n",
      "Epoch 32100, Loss= 20.2288, Training Error= 0.639\n",
      "Epoch 32200, Loss= 20.0846, Training Error= 0.619\n",
      "Epoch 32300, Loss= 19.9455, Training Error= 0.607\n",
      "Epoch 32400, Loss= 19.8094, Training Error= 0.589\n",
      "Epoch 32500, Loss= 19.6774, Training Error= 0.573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32600, Loss= 19.5493, Training Error= 0.559\n",
      "Epoch 32700, Loss= 19.4256, Training Error= 0.546\n",
      "Epoch 32800, Loss= 19.3064, Training Error= 0.530\n",
      "Epoch 32900, Loss= 19.1907, Training Error= 0.515\n",
      "Epoch 33000, Loss= 19.0782, Training Error= 0.502\n",
      "Epoch 33100, Loss= 18.9708, Training Error= 0.489\n",
      "Epoch 33200, Loss= 18.8597, Training Error= 0.483\n",
      "Epoch 33300, Loss= 18.7431, Training Error= 0.480\n",
      "Epoch 33400, Loss= 18.6364, Training Error= 0.470\n",
      "Epoch 33500, Loss= 18.5346, Training Error= 0.462\n",
      "Epoch 33600, Loss= 18.4373, Training Error= 0.452\n",
      "Epoch 33700, Loss= 18.3237, Training Error= 0.439\n",
      "Epoch 33800, Loss= 18.2273, Training Error= 0.426\n",
      "Epoch 33900, Loss= 18.1316, Training Error= 0.419\n",
      "Epoch 34000, Loss= 18.0405, Training Error= 0.414\n",
      "Epoch 34100, Loss= 17.9506, Training Error= 0.410\n",
      "Epoch 34200, Loss= 17.8623, Training Error= 0.403\n",
      "Epoch 34300, Loss= 17.7736, Training Error= 0.394\n",
      "Epoch 34400, Loss= 17.6856, Training Error= 0.387\n",
      "Epoch 34500, Loss= 17.5967, Training Error= 0.377\n",
      "Epoch 34600, Loss= 17.5085, Training Error= 0.381\n",
      "Epoch 34700, Loss= 17.4173, Training Error= 0.372\n",
      "Epoch 34800, Loss= 17.3255, Training Error= 0.367\n",
      "Epoch 34900, Loss= 17.2326, Training Error= 0.356\n",
      "Epoch 35000, Loss= 217.5751, Training Error= 2.729\n",
      "Epoch 35100, Loss= 69.4184, Training Error= 0.206\n",
      "Epoch 35200, Loss= 44.4340, Training Error= 0.502\n",
      "Epoch 35300, Loss= 34.6699, Training Error= 0.390\n",
      "Epoch 35400, Loss= 28.9377, Training Error= 0.276\n",
      "Epoch 35500, Loss= 26.3686, Training Error= 0.275\n",
      "Epoch 35600, Loss= 24.8730, Training Error= 0.260\n",
      "Epoch 35700, Loss= 23.8941, Training Error= 0.224\n",
      "Epoch 35800, Loss= 23.1919, Training Error= 0.197\n",
      "Epoch 35900, Loss= 22.6579, Training Error= 0.178\n",
      "Epoch 36000, Loss= 22.2315, Training Error= 0.165\n",
      "Epoch 36100, Loss= 21.8740, Training Error= 0.155\n",
      "Epoch 36200, Loss= 21.4406, Training Error= 0.150\n",
      "Epoch 36300, Loss= 21.0044, Training Error= 0.130\n",
      "Epoch 36400, Loss= 20.5173, Training Error= 0.116\n",
      "Epoch 36500, Loss= 20.2600, Training Error= 0.110\n",
      "Epoch 36600, Loss= 20.0606, Training Error= 0.106\n",
      "Epoch 36700, Loss= 19.8883, Training Error= 0.102\n",
      "Epoch 36800, Loss= 19.7419, Training Error= 0.109\n",
      "Epoch 36900, Loss= 19.6113, Training Error= 0.104\n",
      "Epoch 37000, Loss= 19.5041, Training Error= 0.094\n",
      "Epoch 37100, Loss= 19.4159, Training Error= 0.090\n",
      "Epoch 37200, Loss= 19.3377, Training Error= 0.087\n",
      "Epoch 37300, Loss= 19.2688, Training Error= 0.090\n",
      "Epoch 37400, Loss= 19.2043, Training Error= 0.077\n",
      "Epoch 37500, Loss= 19.1417, Training Error= 0.082\n",
      "Epoch 37600, Loss= 19.0846, Training Error= 0.075\n",
      "Epoch 37700, Loss= 19.0169, Training Error= 0.074\n",
      "Epoch 37800, Loss= 18.9561, Training Error= 0.073\n",
      "Epoch 37900, Loss= 18.9000, Training Error= 0.071\n",
      "Epoch 38000, Loss= 18.8485, Training Error= 0.074\n",
      "Epoch 38100, Loss= 18.7977, Training Error= 0.068\n",
      "Epoch 38200, Loss= 18.7513, Training Error= 0.071\n",
      "Epoch 38300, Loss= 18.7060, Training Error= 0.072\n",
      "Epoch 38400, Loss= 18.6667, Training Error= 0.078\n",
      "Epoch 38500, Loss= 18.6274, Training Error= 0.068\n",
      "Epoch 38600, Loss= 18.5958, Training Error= 0.074\n",
      "Epoch 38700, Loss= 18.5684, Training Error= 0.080\n",
      "Epoch 38800, Loss= 18.5426, Training Error= 0.049\n",
      "Epoch 38900, Loss= 18.5171, Training Error= 0.068\n",
      "Epoch 39000, Loss= 18.4948, Training Error= 0.035\n",
      "Epoch 39100, Loss= 18.4710, Training Error= 0.077\n",
      "Epoch 39200, Loss= 18.4613, Training Error= 0.013\n",
      "Epoch 39300, Loss= 18.4342, Training Error= 0.007\n",
      "Epoch 39400, Loss= 18.4181, Training Error= 0.007\n",
      "Epoch 39500, Loss= 18.4081, Training Error= 0.147\n",
      "Epoch 39600, Loss= 18.3681, Training Error= 0.051\n",
      "Epoch 39700, Loss= 18.3854, Training Error= 0.175\n",
      "Epoch 39800, Loss= 18.3377, Training Error= 0.005\n",
      "Epoch 39900, Loss= 18.3140, Training Error= 0.065\n",
      "Epoch 40000, Loss= 18.3114, Training Error= 0.133\n",
      "Epoch 40100, Loss= 18.3064, Training Error= 0.158\n",
      "Epoch 40200, Loss= 18.2757, Training Error= 0.003\n",
      "Epoch 40300, Loss= 18.2881, Training Error= 0.058\n",
      "Epoch 40400, Loss= 18.2370, Training Error= 0.077\n",
      "Epoch 40500, Loss= 18.2244, Training Error= 0.080\n",
      "Epoch 40600, Loss= 18.2109, Training Error= 0.080\n",
      "Epoch 40700, Loss= 18.1955, Training Error= 0.063\n",
      "Epoch 40800, Loss= 18.1911, Training Error= 0.123\n",
      "Epoch 40900, Loss= 18.1710, Training Error= 0.079\n",
      "Epoch 41000, Loss= 18.1903, Training Error= 0.038\n",
      "Epoch 41100, Loss= 18.1527, Training Error= 0.112\n",
      "Epoch 41200, Loss= 18.1803, Training Error= 0.065\n",
      "Epoch 41300, Loss= 18.1378, Training Error= 0.022\n",
      "Epoch 41400, Loss= 18.1128, Training Error= 0.065\n",
      "Epoch 41500, Loss= 18.1566, Training Error= 0.154\n",
      "Epoch 41600, Loss= 18.1265, Training Error= 0.141\n",
      "Epoch 41700, Loss= 18.0868, Training Error= 0.006\n",
      "Epoch 41800, Loss= 18.1042, Training Error= 0.059\n",
      "Epoch 41900, Loss= 18.0609, Training Error= 0.089\n",
      "Epoch 42000, Loss= 18.0522, Training Error= 0.005\n",
      "Epoch 42100, Loss= 18.0396, Training Error= 0.093\n",
      "Epoch 42200, Loss= 18.0686, Training Error= 0.058\n",
      "Epoch 42300, Loss= 18.0267, Training Error= 0.106\n",
      "Epoch 42400, Loss= 18.0356, Training Error= 0.114\n",
      "Epoch 42500, Loss= 18.0113, Training Error= 0.028\n",
      "Epoch 42600, Loss= 18.0191, Training Error= 0.119\n",
      "Epoch 42700, Loss= 17.9860, Training Error= 0.110\n",
      "Epoch 42800, Loss= 17.9911, Training Error= 0.117\n",
      "Epoch 42900, Loss= 18.0049, Training Error= 0.069\n",
      "Epoch 43000, Loss= 18.0016, Training Error= 0.074\n",
      "Epoch 43100, Loss= 17.9045, Training Error= 0.043\n",
      "Epoch 43200, Loss= 17.8960, Training Error= 0.004\n",
      "Epoch 43300, Loss= 17.8749, Training Error= 0.076\n",
      "Epoch 43400, Loss= 17.8653, Training Error= 0.076\n",
      "Epoch 43500, Loss= 17.8410, Training Error= 0.068\n",
      "Epoch 43600, Loss= 17.8333, Training Error= 0.079\n",
      "Epoch 43700, Loss= 17.8182, Training Error= 0.076\n",
      "Epoch 43800, Loss= 17.7868, Training Error= 0.040\n",
      "Epoch 43900, Loss= 17.7676, Training Error= 0.055\n",
      "Epoch 44000, Loss= 17.7751, Training Error= 0.058\n",
      "Epoch 44100, Loss= 17.7333, Training Error= 0.043\n",
      "Epoch 44200, Loss= 17.7022, Training Error= 0.072\n",
      "Epoch 44300, Loss= 17.6852, Training Error= 0.057\n",
      "Epoch 44400, Loss= 17.6725, Training Error= 0.056\n",
      "Epoch 44500, Loss= 17.6531, Training Error= 0.046\n",
      "Epoch 44600, Loss= 17.6068, Training Error= 0.069\n",
      "Epoch 44700, Loss= 17.6358, Training Error= 0.033\n",
      "Epoch 44800, Loss= 17.5701, Training Error= 0.028\n",
      "Epoch 44900, Loss= 17.5676, Training Error= 0.003\n",
      "Epoch 45000, Loss= 17.5277, Training Error= 0.069\n",
      "Epoch 45100, Loss= 17.5653, Training Error= 0.036\n",
      "Epoch 45200, Loss= 17.5275, Training Error= 0.058\n",
      "Epoch 45300, Loss= 17.5006, Training Error= 0.050\n",
      "Epoch 45400, Loss= 17.6276, Training Error= 0.001\n",
      "Epoch 45500, Loss= 17.4594, Training Error= 0.067\n",
      "Epoch 45600, Loss= 262.2446, Training Error= 10.602\n",
      "Epoch 45700, Loss= 20.2239, Training Error= 0.309\n",
      "Epoch 45800, Loss= 18.4738, Training Error= 0.061\n",
      "Epoch 45900, Loss= 18.4727, Training Error= 0.061\n",
      "Epoch 46000, Loss= 18.4721, Training Error= 0.060\n",
      "Epoch 46100, Loss= 18.4716, Training Error= 0.061\n",
      "Epoch 46200, Loss= 18.4711, Training Error= 0.061\n",
      "Epoch 46300, Loss= 18.4707, Training Error= 0.061\n",
      "Epoch 46400, Loss= 18.4704, Training Error= 0.061\n",
      "Epoch 46500, Loss= 18.4700, Training Error= 0.061\n",
      "Epoch 46600, Loss= 18.4697, Training Error= 0.061\n",
      "Epoch 46700, Loss= 18.4694, Training Error= 0.061\n",
      "Epoch 46800, Loss= 18.4691, Training Error= 0.061\n",
      "Epoch 46900, Loss= 18.4689, Training Error= 0.060\n",
      "Epoch 47000, Loss= 18.4686, Training Error= 0.060\n",
      "Epoch 47100, Loss= 18.4683, Training Error= 0.059\n",
      "Epoch 47200, Loss= 18.4681, Training Error= 0.060\n",
      "Epoch 47300, Loss= 18.4678, Training Error= 0.060\n",
      "Epoch 47400, Loss= 18.4675, Training Error= 0.060\n",
      "Epoch 47500, Loss= 18.4672, Training Error= 0.060\n",
      "Epoch 47600, Loss= 18.4668, Training Error= 0.060\n",
      "Epoch 47700, Loss= 18.4665, Training Error= 0.059\n",
      "Epoch 47800, Loss= 18.4661, Training Error= 0.059\n",
      "Epoch 47900, Loss= 18.4657, Training Error= 0.059\n",
      "Epoch 48000, Loss= 18.4653, Training Error= 0.060\n",
      "Epoch 48100, Loss= 18.4649, Training Error= 0.061\n",
      "Epoch 48200, Loss= 18.4644, Training Error= 0.060\n",
      "Epoch 48300, Loss= 18.4640, Training Error= 0.061\n",
      "Epoch 48400, Loss= 18.4634, Training Error= 0.060\n",
      "Epoch 48500, Loss= 18.4628, Training Error= 0.061\n",
      "Epoch 48600, Loss= 18.4622, Training Error= 0.061\n",
      "Epoch 48700, Loss= 18.4616, Training Error= 0.062\n",
      "Epoch 48800, Loss= 18.4609, Training Error= 0.060\n",
      "Epoch 48900, Loss= 18.4602, Training Error= 0.060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49000, Loss= 18.4851, Training Error= 0.166\n",
      "Epoch 49100, Loss= 18.4586, Training Error= 0.058\n",
      "Epoch 49200, Loss= 18.4580, Training Error= 0.054\n",
      "Epoch 49300, Loss= 18.4649, Training Error= 0.023\n",
      "Epoch 49400, Loss= 18.4834, Training Error= 0.157\n",
      "Epoch 49500, Loss= 18.4550, Training Error= 0.073\n",
      "Epoch 49600, Loss= 18.4541, Training Error= 0.057\n",
      "Epoch 49700, Loss= 18.4891, Training Error= 0.032\n",
      "Epoch 49800, Loss= 18.4526, Training Error= 0.042\n",
      "Epoch 49900, Loss= 18.4533, Training Error= 0.093\n",
      "[[1.6417689 3.5444336]\n",
      " [1.8874831 3.0291078]\n",
      " [1.5248743 3.818388 ]\n",
      " ...\n",
      " [1.677586  3.4644413]\n",
      " [1.8773675 3.0488617]\n",
      " [1.5222211 3.824846 ]]\n",
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "if train_model:\n",
    "\n",
    "    # Initialize the variables (i.e. assign their default value)\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # Run initializer\n",
    "        sess.run(init)\n",
    "        for epoch in range(num_epochs):\n",
    "            sess.run(train_op, feed_dict={X: train_x, Y: train_y})\n",
    "\n",
    "            if epoch % display_epoch == 0 or epoch == 1:\n",
    "                loss, acc = sess.run([loss_op, accuracy], feed_dict={X: train_x, Y: train_y})\n",
    "\n",
    "                print(\"Epoch \" + str(epoch) + \", Loss= \" + \\\n",
    "                      \"{:.4f}\".format(loss) + \", Training Error= \" + \\\n",
    "                      \"{:.3f}\".format(acc))\n",
    "\n",
    "\n",
    "        print(sess.run(output, feed_dict={X: train_x}))\n",
    "        saver.save(sess, model_dir + stock_symbol + '_regularizer.ckpt')\n",
    "        \n",
    "        \n",
    "        print('Model Saved')\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./regularizer_models/c/c_regularizer.ckpt\n",
      "Model restored.\n"
     ]
    }
   ],
   "source": [
    "if test_model:\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        '''var_name_list = [v.name for v in tf.trainable_variables()]\n",
    "        print(var_name_list)\n",
    "        \n",
    "        reader = pywrap_tensorflow.NewCheckpointReader(model_dir + stock_symbol + '_regularizer.ckpt')\n",
    "        var_to_shape_map = reader.get_variable_to_shape_map()'''\n",
    "        \n",
    "        saver.restore(sess, model_dir + stock_symbol + '_regularizer.ckpt')\n",
    "        print(\"Model restored.\")\n",
    "        predicted = tf.round(sess.run(output, feed_dict={X : test_x})).eval()\n",
    "        predicted = np.array(predicted)\n",
    "        \n",
    "    result = pd.DataFrame(predicted, columns = y_columns)\n",
    "    result['Date_to_use_on'] = test_data['Date_to_use_on']\n",
    "    result = result[['Date_to_use_on'] + y_columns]\n",
    "    result.to_csv(training_data_dir+'c_test_parameters.csv', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date_to_use_on</th>\n",
       "      <th>ppo</th>\n",
       "      <th>rsi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-11-20</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-11-23</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-11-24</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-11-25</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-11-27</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2009-11-30</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2009-12-01</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2009-12-02</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2009-12-03</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2009-12-04</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2009-12-07</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2009-12-08</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2009-12-09</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2009-12-10</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2009-12-11</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2009-12-14</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2009-12-15</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2009-12-16</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2009-12-17</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2009-12-18</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2009-12-21</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2009-12-22</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2009-12-23</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2009-12-24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2009-12-28</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2009-12-29</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2009-12-30</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2009-12-31</td>\n",
       "      <td>20.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2010-01-05</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>2012-06-14</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>2012-06-15</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>2012-06-18</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648</th>\n",
       "      <td>2012-06-19</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>2012-06-20</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650</th>\n",
       "      <td>2012-06-21</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651</th>\n",
       "      <td>2012-06-22</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652</th>\n",
       "      <td>2012-06-25</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653</th>\n",
       "      <td>2012-06-26</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>654</th>\n",
       "      <td>2012-06-27</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>655</th>\n",
       "      <td>2012-06-28</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656</th>\n",
       "      <td>2012-06-29</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>657</th>\n",
       "      <td>2012-07-02</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658</th>\n",
       "      <td>2012-07-03</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>659</th>\n",
       "      <td>2012-07-05</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>2012-07-06</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661</th>\n",
       "      <td>2012-07-09</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>2012-07-10</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>663</th>\n",
       "      <td>2012-07-11</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664</th>\n",
       "      <td>2012-07-12</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>2012-07-13</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>666</th>\n",
       "      <td>2012-07-16</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>667</th>\n",
       "      <td>2012-07-17</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668</th>\n",
       "      <td>2012-07-18</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669</th>\n",
       "      <td>2012-07-19</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>2012-07-20</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>2012-07-23</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>2012-07-24</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>2012-07-25</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>2012-07-26</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>675 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Date_to_use_on   ppo   rsi\n",
       "0       2009-11-20   2.0   3.0\n",
       "1       2009-11-23   2.0   3.0\n",
       "2       2009-11-24   2.0   3.0\n",
       "3       2009-11-25   2.0   3.0\n",
       "4       2009-11-27   2.0   3.0\n",
       "5       2009-11-30   2.0   3.0\n",
       "6       2009-12-01   2.0   3.0\n",
       "7       2009-12-02   1.0   4.0\n",
       "8       2009-12-03   1.0   4.0\n",
       "9       2009-12-04   1.0   4.0\n",
       "10      2009-12-07   2.0   4.0\n",
       "11      2009-12-08   2.0   4.0\n",
       "12      2009-12-09   2.0   3.0\n",
       "13      2009-12-10   2.0   3.0\n",
       "14      2009-12-11   2.0   3.0\n",
       "15      2009-12-14   2.0   3.0\n",
       "16      2009-12-15   2.0   3.0\n",
       "17      2009-12-16   2.0   3.0\n",
       "18      2009-12-17   2.0   3.0\n",
       "19      2009-12-18   4.0   1.0\n",
       "20      2009-12-21   8.0   0.0\n",
       "21      2009-12-22   2.0   3.0\n",
       "22      2009-12-23   3.0   2.0\n",
       "23      2009-12-24   0.0  24.0\n",
       "24      2009-12-28   2.0   2.0\n",
       "25      2009-12-29   6.0   0.0\n",
       "26      2009-12-30  11.0   2.0\n",
       "27      2009-12-31  20.0   9.0\n",
       "28      2010-01-04   6.0   0.0\n",
       "29      2010-01-05   5.0   0.0\n",
       "..             ...   ...   ...\n",
       "645     2012-06-14   2.0   4.0\n",
       "646     2012-06-15   2.0   3.0\n",
       "647     2012-06-18   2.0   3.0\n",
       "648     2012-06-19   2.0   3.0\n",
       "649     2012-06-20   2.0   3.0\n",
       "650     2012-06-21   2.0   3.0\n",
       "651     2012-06-22   2.0   3.0\n",
       "652     2012-06-25   2.0   3.0\n",
       "653     2012-06-26   2.0   3.0\n",
       "654     2012-06-27   2.0   3.0\n",
       "655     2012-06-28   2.0   3.0\n",
       "656     2012-06-29   2.0   3.0\n",
       "657     2012-07-02   2.0   3.0\n",
       "658     2012-07-03   2.0   3.0\n",
       "659     2012-07-05   2.0   3.0\n",
       "660     2012-07-06   2.0   3.0\n",
       "661     2012-07-09   2.0   3.0\n",
       "662     2012-07-10   2.0   3.0\n",
       "663     2012-07-11   2.0   3.0\n",
       "664     2012-07-12   2.0   3.0\n",
       "665     2012-07-13   2.0   3.0\n",
       "666     2012-07-16   2.0   3.0\n",
       "667     2012-07-17   2.0   3.0\n",
       "668     2012-07-18   2.0   3.0\n",
       "669     2012-07-19   2.0   3.0\n",
       "670     2012-07-20   2.0   3.0\n",
       "671     2012-07-23   2.0   3.0\n",
       "672     2012-07-24   2.0   3.0\n",
       "673     2012-07-25   2.0   3.0\n",
       "674     2012-07-26   2.0   3.0\n",
       "\n",
       "[675 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
