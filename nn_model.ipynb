{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, SimpleRNN, Dense, Dropout, Lambda\n",
    "from keras.callbacks import Callback\n",
    "import keras\n",
    "from tensorflow.python import pywrap_tensorflow\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "PARAMETERS TO CHANGE\n",
    "'''\n",
    "stock_symbol = 'bac'\n",
    "train_model = True\n",
    "test_model = True\n",
    "learning_rate = 0.1\n",
    "''''''\n",
    "\n",
    "training_data_dir = './training_data/'\n",
    "y_columns = ['ppo', 'rsi']\n",
    "#assert(train_model ^ test_model)\n",
    "model_dir = \"./regularizer_models/\" + stock_symbol + '/'\n",
    "\n",
    "if train_model:\n",
    "    training_data = pd.read_csv(training_data_dir + stock_symbol + '_training_data_smoothed.csv')\n",
    "    unneeded_columns = ['window', 'Date_to_use_on']\n",
    "\n",
    "    x_columns = list(training_data.columns)\n",
    "    for to_remove in (y_columns + unneeded_columns):\n",
    "        x_columns.remove(to_remove)\n",
    "\n",
    "    train_x = training_data[x_columns].values\n",
    "    train_y = training_data[y_columns].values\n",
    "\n",
    "    model_dir = \"./regularizer_models/\" + stock_symbol + '/'\n",
    "\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "        \n",
    "if test_model:\n",
    "    test_data = pd.read_csv(training_data_dir + stock_symbol + '_test_data.csv')\n",
    "    \n",
    "    unneeded_columns = ['Date_to_use_on']\n",
    "    x_columns = list(test_data.columns)\n",
    "\n",
    "    for to_remove in (unneeded_columns):\n",
    "        x_columns.remove(to_remove)\n",
    "\n",
    "    \n",
    "    test_x = test_data[x_columns].values\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\indexing.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "566\n",
      "1981\n",
      "2025\n"
     ]
    }
   ],
   "source": [
    "'''data = pd.read_csv(training_data_dir + stock_symbol + '_training_data.csv').sort_values(\"window\")\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    if index > 0 and index < data.shape[0] - 1:\n",
    "        prev_row = data.iloc[index-1]\n",
    "        next_row = data.iloc[index+1]\n",
    "        if row['ppo'] != prev_row['ppo'] and row['rsi'] != prev_row['rsi']:\n",
    "            if prev_row['ppo'] == next_row['ppo'] and prev_row['rsi'] == next_row['rsi']:\n",
    "                print(index)\n",
    "                data['ppo'].loc[index] = prev_row['ppo']\n",
    "                data['rsi'].loc[index] = prev_row['rsi']\n",
    "                \n",
    "data.to_csv(training_data_dir + stock_symbol + '_training_data_smoothed.csv', index=False)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Parameters\\n\\nnum_epochs = 30000\\nbatch_size = 2075\\ndisplay_epoch = 100\\n\\n# Network Parameters\\nn_hidden_1 = 256 # 1st layer number of neurons\\nn_hidden_2 = 256 # 2nd layer number of neurons\\nnum_input = len(x_columns) # Number of features\\nnum_output = len(y_columns) # PPO, RSI parameters\\n\\n\\ndef ANN(X):\\n    # Store layers weight & bias\\n    weights = {\\n        \\'h1\\': tf.Variable(tf.random_normal([num_input, n_hidden_1])),\\n        \\'h2\\': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\\n        \\'out\\': tf.Variable(tf.random_normal([n_hidden_2, num_output]))\\n    }\\n    biases = {\\n        \\'b1\\': tf.Variable(tf.random_normal([n_hidden_1])),\\n        \\'b2\\': tf.Variable(tf.random_normal([n_hidden_2])),\\n        \\'out\\': tf.Variable(tf.random_normal([num_output]))\\n    }\\n\\n    # Hidden fully connected layer with 256 neurons\\n    layer_1 = tf.add(tf.matmul(X, weights[\\'h1\\']), biases[\\'b1\\'])\\n    layer_1 = tf.nn.relu(layer_1)    \\n    \\n    # Hidden fully connected layer with 256 neurons\\n    layer_2 = tf.add(tf.matmul(layer_1, weights[\\'h2\\']), biases[\\'b2\\'])\\n    layer_2 = tf.nn.relu(layer_2)\\n    \\n    \\n    # Output fully connected layer with a neuron for each output\\n    out_layer = tf.matmul(layer_2, weights[\\'out\\']) + biases[\\'out\\']\\n    out = tf.square(out_layer)\\n    \\n    return out\\n\\nX = tf.placeholder(\"float\", [None, num_input])\\nY = tf.placeholder(\"float\", [None, num_output])\\n\\noutput = ANN(X)\\n\\n# Loss & Optimizer\\nloss_op = tf.losses.mean_squared_error(Y, output, weights=1)\\ntrain_op = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss_op)\\n# Accuracy\\naccuracy =  tf.abs(tf.reduce_mean(tf.round(output) - Y))'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Parameters\n",
    "\n",
    "num_epochs = 30000\n",
    "batch_size = 2075\n",
    "display_epoch = 100\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of neurons\n",
    "n_hidden_2 = 256 # 2nd layer number of neurons\n",
    "num_input = len(x_columns) # Number of features\n",
    "num_output = len(y_columns) # PPO, RSI parameters\n",
    "\n",
    "\n",
    "def ANN(X):\n",
    "    # Store layers weight & bias\n",
    "    weights = {\n",
    "        'h1': tf.Variable(tf.random_normal([num_input, n_hidden_1])),\n",
    "        'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden_2, num_output]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "        'out': tf.Variable(tf.random_normal([num_output]))\n",
    "    }\n",
    "\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_1 = tf.add(tf.matmul(X, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)    \n",
    "    \n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    \n",
    "    \n",
    "    # Output fully connected layer with a neuron for each output\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    out = tf.square(out_layer)\n",
    "    \n",
    "    return out\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_output])\n",
    "\n",
    "output = ANN(X)\n",
    "\n",
    "# Loss & Optimizer\n",
    "loss_op = tf.losses.mean_squared_error(Y, output, weights=1)\n",
    "train_op = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss_op)\n",
    "# Accuracy\n",
    "accuracy =  tf.abs(tf.reduce_mean(tf.round(output) - Y))'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " - 6s - loss: 30.4082 - accuracy: 3.5178\n",
      "Epoch 2/200\n",
      " - 0s - loss: 30.4061 - accuracy: 3.5178\n",
      "Epoch 3/200\n",
      " - 0s - loss: 30.4033 - accuracy: 3.5178\n",
      "Epoch 4/200\n",
      " - 0s - loss: 30.3999 - accuracy: 3.5178\n",
      "Epoch 5/200\n",
      " - 0s - loss: 30.3976 - accuracy: 3.5178\n",
      "Epoch 6/200\n",
      " - 0s - loss: 30.3930 - accuracy: 3.5178\n",
      "Epoch 7/200\n",
      " - 0s - loss: 30.3869 - accuracy: 3.5178\n",
      "Epoch 8/200\n",
      " - 0s - loss: 30.3792 - accuracy: 3.5178\n",
      "Epoch 9/200\n",
      " - 0s - loss: 30.3703 - accuracy: 3.5178\n",
      "Epoch 10/200\n",
      " - 0s - loss: 30.3570 - accuracy: 3.5178\n",
      "Epoch 11/200\n",
      " - 0s - loss: 30.3484 - accuracy: 3.5178\n",
      "Epoch 12/200\n",
      " - 0s - loss: 30.3398 - accuracy: 3.5178\n",
      "Epoch 13/200\n",
      " - 0s - loss: 30.3199 - accuracy: 3.5178\n",
      "Epoch 14/200\n",
      " - 0s - loss: 30.2977 - accuracy: 3.5178\n",
      "Epoch 15/200\n",
      " - 0s - loss: 30.2789 - accuracy: 3.5178\n",
      "Epoch 16/200\n",
      " - 0s - loss: 30.2318 - accuracy: 3.5178\n",
      "Epoch 17/200\n",
      " - 0s - loss: 30.1970 - accuracy: 3.5173\n",
      "Epoch 18/200\n",
      " - 0s - loss: 30.1638 - accuracy: 3.5166\n",
      "Epoch 19/200\n",
      " - 0s - loss: 30.0951 - accuracy: 3.5154\n",
      "Epoch 20/200\n",
      " - 0s - loss: 30.0387 - accuracy: 3.5154\n",
      "Epoch 21/200\n",
      " - 0s - loss: 29.9358 - accuracy: 3.5118\n",
      "Epoch 22/200\n",
      " - 0s - loss: 29.8365 - accuracy: 3.5051\n",
      "Epoch 23/200\n",
      " - 0s - loss: 29.7766 - accuracy: 3.5005\n",
      "Epoch 24/200\n",
      " - 0s - loss: 29.6069 - accuracy: 3.4877\n",
      "Epoch 25/200\n",
      " - 0s - loss: 29.5011 - accuracy: 3.4754\n",
      "Epoch 26/200\n",
      " - 0s - loss: 29.3432 - accuracy: 3.4581\n",
      "Epoch 27/200\n",
      " - 0s - loss: 29.0512 - accuracy: 3.4357\n",
      "Epoch 28/200\n",
      " - 0s - loss: 28.9065 - accuracy: 3.3973\n",
      "Epoch 29/200\n",
      " - 0s - loss: 28.8221 - accuracy: 3.3776\n",
      "Epoch 30/200\n",
      " - 0s - loss: 28.4007 - accuracy: 3.3320\n",
      "Epoch 31/200\n",
      " - 0s - loss: 27.9535 - accuracy: 3.2720\n",
      "Epoch 32/200\n",
      " - 0s - loss: 27.8293 - accuracy: 3.2243\n",
      "Epoch 33/200\n",
      " - 0s - loss: 27.5520 - accuracy: 3.1619\n",
      "Epoch 34/200\n",
      " - 0s - loss: 26.9346 - accuracy: 3.1113\n",
      "Epoch 35/200\n",
      " - 0s - loss: 26.6234 - accuracy: 3.0320\n",
      "Epoch 36/200\n",
      " - 0s - loss: 26.0871 - accuracy: 2.9627\n",
      "Epoch 37/200\n",
      " - 0s - loss: 25.8806 - accuracy: 2.8720\n",
      "Epoch 38/200\n",
      " - 0s - loss: 25.1684 - accuracy: 2.7716\n",
      "Epoch 39/200\n",
      " - 0s - loss: 24.9053 - accuracy: 2.7142\n",
      "Epoch 40/200\n",
      " - 0s - loss: 24.1361 - accuracy: 2.5740\n",
      "Epoch 41/200\n",
      " - 0s - loss: 23.7316 - accuracy: 2.4602\n",
      "Epoch 42/200\n",
      " - 0s - loss: 23.5128 - accuracy: 2.3313\n",
      "Epoch 43/200\n",
      " - 0s - loss: 22.7318 - accuracy: 2.1928\n",
      "Epoch 44/200\n",
      " - 0s - loss: 22.0341 - accuracy: 2.0489\n",
      "Epoch 45/200\n",
      " - 0s - loss: 21.4962 - accuracy: 1.8790\n",
      "Epoch 46/200\n",
      " - 0s - loss: 21.2383 - accuracy: 1.7949\n",
      "Epoch 47/200\n",
      " - 0s - loss: 20.8250 - accuracy: 1.6113\n",
      "Epoch 48/200\n",
      " - 0s - loss: 20.0770 - accuracy: 1.4894\n",
      "Epoch 49/200\n",
      " - 0s - loss: 19.4232 - accuracy: 1.3340\n",
      "Epoch 50/200\n",
      " - 0s - loss: 19.0471 - accuracy: 1.1113\n",
      "Epoch 51/200\n",
      " - 0s - loss: 18.6524 - accuracy: 0.9583\n",
      "Epoch 52/200\n",
      " - 0s - loss: 18.7838 - accuracy: 0.8595\n",
      "Epoch 53/200\n",
      " - 0s - loss: 18.4466 - accuracy: 0.6875\n",
      "Epoch 54/200\n",
      " - 0s - loss: 18.4111 - accuracy: 0.5446\n",
      "Epoch 55/200\n",
      " - 0s - loss: 18.1753 - accuracy: 0.3511\n",
      "Epoch 56/200\n",
      " - 0s - loss: 18.0427 - accuracy: 0.2383\n",
      "Epoch 57/200\n",
      " - 0s - loss: 17.7654 - accuracy: 0.1258\n",
      "Epoch 58/200\n",
      " - 0s - loss: 18.0593 - accuracy: 0.0393\n",
      "Epoch 59/200\n",
      " - 0s - loss: 18.0928 - accuracy: 0.0492\n",
      "Epoch 60/200\n",
      " - 0s - loss: 17.8879 - accuracy: 0.1275\n",
      "Epoch 61/200\n",
      " - 0s - loss: 17.2852 - accuracy: 0.1612\n",
      "Epoch 62/200\n",
      " - 0s - loss: 17.5701 - accuracy: 0.2349\n",
      "Epoch 63/200\n",
      " - 0s - loss: 17.7513 - accuracy: 0.2041\n",
      "Epoch 64/200\n",
      " - 0s - loss: 17.6029 - accuracy: 0.2436\n",
      "Epoch 65/200\n",
      " - 0s - loss: 17.6243 - accuracy: 0.2145\n",
      "Epoch 66/200\n",
      " - 0s - loss: 17.7127 - accuracy: 0.1677\n",
      "Epoch 67/200\n",
      " - 0s - loss: 17.4314 - accuracy: 0.1870\n",
      "Epoch 68/200\n",
      " - 0s - loss: 17.0890 - accuracy: 0.1036\n",
      "Epoch 69/200\n",
      " - 0s - loss: 16.8801 - accuracy: 0.1176\n",
      "Epoch 70/200\n",
      " - 0s - loss: 17.0826 - accuracy: 0.0369\n",
      "Epoch 71/200\n",
      " - 0s - loss: 17.3544 - accuracy: 0.0359\n",
      "Epoch 72/200\n",
      " - 0s - loss: 17.1291 - accuracy: 0.0359\n",
      "Epoch 73/200\n",
      " - 0s - loss: 16.8360 - accuracy: 0.0467\n",
      "Epoch 74/200\n",
      " - 0s - loss: 16.8191 - accuracy: 0.0880\n",
      "Epoch 75/200\n",
      " - 0s - loss: 16.5937 - accuracy: 0.1571\n",
      "Epoch 76/200\n",
      " - 0s - loss: 17.0786 - accuracy: 0.1552\n",
      "Epoch 77/200\n",
      " - 0s - loss: 16.9636 - accuracy: 0.2000\n",
      "Epoch 78/200\n",
      " - 0s - loss: 16.7866 - accuracy: 0.2019\n",
      "Epoch 79/200\n",
      " - 0s - loss: 16.9742 - accuracy: 0.2455\n",
      "Epoch 80/200\n",
      " - 0s - loss: 17.0936 - accuracy: 0.2757\n",
      "Epoch 81/200\n",
      " - 0s - loss: 16.9028 - accuracy: 0.2757\n",
      "Epoch 82/200\n",
      " - 0s - loss: 16.7458 - accuracy: 0.2665\n",
      "Epoch 83/200\n",
      " - 0s - loss: 16.9090 - accuracy: 0.2918\n",
      "Epoch 84/200\n",
      " - 0s - loss: 16.8401 - accuracy: 0.3075\n",
      "Epoch 85/200\n",
      " - 0s - loss: 16.8115 - accuracy: 0.2617\n",
      "Epoch 86/200\n",
      " - 0s - loss: 16.7728 - accuracy: 0.2867\n",
      "Epoch 87/200\n",
      " - 0s - loss: 16.8606 - accuracy: 0.2675\n",
      "Epoch 88/200\n",
      " - 0s - loss: 16.6850 - accuracy: 0.2733\n",
      "Epoch 89/200\n",
      " - 0s - loss: 16.6763 - accuracy: 0.2472\n",
      "Epoch 90/200\n",
      " - 0s - loss: 16.8715 - accuracy: 0.2513\n",
      "Epoch 91/200\n",
      " - 0s - loss: 16.7556 - accuracy: 0.2489\n",
      "Epoch 92/200\n",
      " - 0s - loss: 16.6678 - accuracy: 0.2255\n",
      "Epoch 93/200\n",
      " - 0s - loss: 16.7060 - accuracy: 0.2120\n",
      "Epoch 94/200\n",
      " - 0s - loss: 16.6504 - accuracy: 0.1925\n",
      "Epoch 95/200\n",
      " - 0s - loss: 16.5566 - accuracy: 0.1923\n",
      "Epoch 96/200\n",
      " - 0s - loss: 16.3358 - accuracy: 0.1624\n",
      "Epoch 97/200\n",
      " - 0s - loss: 16.6734 - accuracy: 0.1677\n",
      "Epoch 98/200\n",
      " - 0s - loss: 16.6985 - accuracy: 0.1246\n",
      "Epoch 99/200\n",
      " - 0s - loss: 16.5514 - accuracy: 0.1422\n",
      "Epoch 100/200\n",
      " - 0s - loss: 16.5614 - accuracy: 0.1349\n",
      "Epoch 101/200\n",
      " - 0s - loss: 16.4558 - accuracy: 0.0795\n",
      "Epoch 102/200\n",
      " - 0s - loss: 16.3798 - accuracy: 0.0660\n",
      "Epoch 103/200\n",
      " - 0s - loss: 16.5212 - accuracy: 0.0590\n",
      "Epoch 104/200\n",
      " - 0s - loss: 16.4744 - accuracy: 0.0672\n",
      "Epoch 105/200\n",
      " - 0s - loss: 16.4398 - accuracy: 0.0884\n",
      "Epoch 106/200\n",
      " - 0s - loss: 16.5238 - accuracy: 0.0569\n",
      "Epoch 107/200\n",
      " - 0s - loss: 16.4512 - accuracy: 0.0523\n",
      "Epoch 108/200\n",
      " - 0s - loss: 16.4556 - accuracy: 0.0545\n",
      "Epoch 109/200\n",
      " - 0s - loss: 16.5812 - accuracy: 0.0422\n",
      "Epoch 110/200\n",
      " - 0s - loss: 16.4740 - accuracy: 0.0359\n",
      "Epoch 111/200\n",
      " - 0s - loss: 16.5550 - accuracy: 0.0680\n",
      "Epoch 112/200\n",
      " - 0s - loss: 16.3983 - accuracy: 0.0643\n",
      "Epoch 113/200\n",
      " - 0s - loss: 16.4183 - accuracy: 0.0612\n",
      "Epoch 114/200\n",
      " - 0s - loss: 16.4826 - accuracy: 0.0723\n",
      "Epoch 115/200\n",
      " - 0s - loss: 16.4643 - accuracy: 0.0501\n",
      "Epoch 116/200\n",
      " - 0s - loss: 16.3835 - accuracy: 0.0612\n",
      "Epoch 117/200\n",
      " - 0s - loss: 16.5249 - accuracy: 0.0757\n",
      "Epoch 118/200\n",
      " - 0s - loss: 16.3907 - accuracy: 0.0928\n",
      "Epoch 119/200\n",
      " - 0s - loss: 16.3167 - accuracy: 0.0776\n",
      "Epoch 120/200\n",
      " - 0s - loss: 16.3545 - accuracy: 0.0798\n",
      "Epoch 121/200\n",
      " - 0s - loss: 16.4723 - accuracy: 0.0867\n",
      "Epoch 122/200\n",
      " - 0s - loss: 16.4226 - accuracy: 0.0887\n",
      "Epoch 123/200\n",
      " - 0s - loss: 16.3683 - accuracy: 0.0853\n",
      "Epoch 124/200\n",
      " - 0s - loss: 16.4825 - accuracy: 0.0998\n",
      "Epoch 125/200\n",
      " - 0s - loss: 16.4766 - accuracy: 0.0933\n",
      "Epoch 126/200\n",
      " - 0s - loss: 16.3285 - accuracy: 0.0894\n",
      "Epoch 127/200\n",
      " - 0s - loss: 16.3003 - accuracy: 0.0986\n",
      "Epoch 128/200\n",
      " - 0s - loss: 16.4648 - accuracy: 0.0978\n",
      "Epoch 129/200\n",
      " - 0s - loss: 16.4271 - accuracy: 0.0971\n",
      "Epoch 130/200\n",
      " - 0s - loss: 16.3182 - accuracy: 0.0933\n",
      "Epoch 131/200\n",
      " - 0s - loss: 16.4072 - accuracy: 0.0928\n",
      "Epoch 132/200\n",
      " - 0s - loss: 16.4369 - accuracy: 0.0918\n",
      "Epoch 133/200\n",
      " - 0s - loss: 16.3891 - accuracy: 0.0995\n",
      "Epoch 134/200\n",
      " - 0s - loss: 16.2405 - accuracy: 0.0846\n",
      "Epoch 135/200\n",
      " - 0s - loss: 16.3713 - accuracy: 0.0966\n",
      "Epoch 136/200\n",
      " - 0s - loss: 16.3175 - accuracy: 0.0841\n",
      "Epoch 137/200\n",
      " - 0s - loss: 16.4469 - accuracy: 0.0834\n",
      "Epoch 138/200\n",
      " - 0s - loss: 16.3590 - accuracy: 0.0872\n",
      "Epoch 139/200\n",
      " - 0s - loss: 16.4398 - accuracy: 0.0858\n",
      "Epoch 140/200\n",
      " - 0s - loss: 16.3901 - accuracy: 0.0737\n",
      "Epoch 141/200\n",
      " - 0s - loss: 16.4755 - accuracy: 0.0788\n",
      "Epoch 142/200\n",
      " - 0s - loss: 16.4091 - accuracy: 0.0614\n",
      "Epoch 143/200\n",
      " - 0s - loss: 16.3811 - accuracy: 0.0855\n",
      "Epoch 144/200\n",
      " - 0s - loss: 16.3442 - accuracy: 0.0737\n",
      "Epoch 145/200\n",
      " - 0s - loss: 16.2663 - accuracy: 0.0646\n",
      "Epoch 146/200\n",
      " - 0s - loss: 16.4425 - accuracy: 0.0711\n",
      "Epoch 147/200\n",
      " - 0s - loss: 16.4061 - accuracy: 0.0718\n",
      "Epoch 148/200\n",
      " - 0s - loss: 16.3934 - accuracy: 0.0831\n",
      "Epoch 149/200\n",
      " - 0s - loss: 16.3414 - accuracy: 0.0542\n",
      "Epoch 150/200\n",
      " - 0s - loss: 16.3538 - accuracy: 0.0745\n",
      "Epoch 151/200\n",
      " - 0s - loss: 16.3525 - accuracy: 0.0670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 152/200\n",
      " - 0s - loss: 16.3801 - accuracy: 0.0643\n",
      "Epoch 153/200\n",
      " - 0s - loss: 16.3683 - accuracy: 0.0602\n",
      "Epoch 154/200\n",
      " - 0s - loss: 16.4088 - accuracy: 0.0745\n",
      "Epoch 155/200\n",
      " - 0s - loss: 16.3158 - accuracy: 0.0629\n",
      "Epoch 156/200\n",
      " - 0s - loss: 16.3542 - accuracy: 0.0684\n",
      "Epoch 157/200\n",
      " - 0s - loss: 16.3404 - accuracy: 0.0723\n",
      "Epoch 158/200\n",
      " - 0s - loss: 16.3124 - accuracy: 0.0687\n",
      "Epoch 159/200\n",
      " - 0s - loss: 16.4659 - accuracy: 0.0708\n",
      "Epoch 160/200\n",
      " - 0s - loss: 16.2526 - accuracy: 0.0586\n",
      "Epoch 161/200\n",
      " - 0s - loss: 16.2439 - accuracy: 0.0812\n",
      "Epoch 162/200\n",
      " - 0s - loss: 16.3613 - accuracy: 0.0520\n",
      "Epoch 163/200\n",
      " - 0s - loss: 16.2843 - accuracy: 0.0653\n",
      "Epoch 164/200\n",
      " - 0s - loss: 16.3507 - accuracy: 0.0639\n",
      "Epoch 165/200\n",
      " - 0s - loss: 16.3474 - accuracy: 0.0634\n",
      "Epoch 166/200\n",
      " - 0s - loss: 16.3284 - accuracy: 0.0692\n",
      "Epoch 167/200\n",
      " - 0s - loss: 16.4157 - accuracy: 0.0680\n",
      "Epoch 168/200\n",
      " - 0s - loss: 16.2562 - accuracy: 0.0651\n",
      "Epoch 169/200\n",
      " - 0s - loss: 16.2636 - accuracy: 0.0513\n",
      "Epoch 170/200\n",
      " - 0s - loss: 16.3223 - accuracy: 0.0694\n",
      "Epoch 171/200\n",
      " - 0s - loss: 16.3329 - accuracy: 0.0501\n",
      "Epoch 172/200\n",
      " - 0s - loss: 16.3695 - accuracy: 0.0511\n",
      "Epoch 173/200\n",
      " - 0s - loss: 16.2088 - accuracy: 0.0475\n",
      "Epoch 174/200\n",
      " - 0s - loss: 16.2374 - accuracy: 0.0518\n",
      "Epoch 175/200\n",
      " - 0s - loss: 16.3001 - accuracy: 0.0566\n",
      "Epoch 176/200\n",
      " - 0s - loss: 16.2892 - accuracy: 0.0559\n",
      "Epoch 177/200\n",
      " - 0s - loss: 16.3290 - accuracy: 0.0569\n",
      "Epoch 178/200\n",
      " - 0s - loss: 16.1816 - accuracy: 0.0561\n",
      "Epoch 179/200\n",
      " - 0s - loss: 16.3219 - accuracy: 0.0595\n",
      "Epoch 180/200\n",
      " - 0s - loss: 16.2664 - accuracy: 0.0508\n",
      "Epoch 181/200\n",
      " - 0s - loss: 16.2813 - accuracy: 0.0547\n",
      "Epoch 182/200\n",
      " - 0s - loss: 16.3059 - accuracy: 0.0583\n",
      "Epoch 183/200\n",
      " - 0s - loss: 16.2995 - accuracy: 0.0516\n",
      "Epoch 184/200\n",
      " - 0s - loss: 16.2857 - accuracy: 0.0451\n",
      "Epoch 185/200\n",
      " - 0s - loss: 16.2590 - accuracy: 0.0518\n",
      "Epoch 186/200\n",
      " - 0s - loss: 16.2798 - accuracy: 0.0535\n",
      "Epoch 187/200\n",
      " - 0s - loss: 16.2939 - accuracy: 0.0494\n",
      "Epoch 188/200\n",
      " - 0s - loss: 16.3700 - accuracy: 0.0429\n",
      "Epoch 189/200\n",
      " - 0s - loss: 16.2660 - accuracy: 0.0470\n",
      "Epoch 190/200\n",
      " - 0s - loss: 16.3213 - accuracy: 0.0499\n",
      "Epoch 191/200\n",
      " - 0s - loss: 16.2082 - accuracy: 0.0639\n",
      "Epoch 192/200\n",
      " - 0s - loss: 16.2543 - accuracy: 0.0465\n",
      "Epoch 193/200\n",
      " - 0s - loss: 16.3085 - accuracy: 0.0533\n",
      "Epoch 194/200\n",
      " - 0s - loss: 16.3351 - accuracy: 0.0458\n",
      "Epoch 195/200\n",
      " - 0s - loss: 16.3915 - accuracy: 0.0467\n",
      "Epoch 196/200\n",
      " - 0s - loss: 16.3004 - accuracy: 0.0520\n",
      "Epoch 197/200\n",
      " - 0s - loss: 16.2175 - accuracy: 0.0484\n",
      "Epoch 198/200\n",
      " - 0s - loss: 16.3402 - accuracy: 0.0453\n",
      "Epoch 199/200\n",
      " - 0s - loss: 16.2706 - accuracy: 0.0545\n",
      "Epoch 200/200\n",
      " - 0s - loss: 16.2886 - accuracy: 0.0496\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21f8b641898>"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EarlyStoppingByLossVal(Callback):\n",
    "    def __init__(self, monitor='loss', value=20, verbose=0):\n",
    "        super(Callback, self).__init__()\n",
    "        self.monitor = monitor\n",
    "        self.value = value\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        current = logs.get(self.monitor)\n",
    "        #if current is None:\n",
    "            #warnings.warn(\"Early stopping requires %s available!\" % self.monitor, RuntimeWarning)\n",
    "\n",
    "        if current < self.value:\n",
    "            if self.verbose > 0:\n",
    "                print(\"Epoch %05d: early stopping THR\" % epoch)\n",
    "            self.model.stop_training = True\n",
    "\n",
    "            \n",
    "def accuracy(y_true, y_pred):\n",
    "    return tf.abs(tf.reduce_mean(tf.round(y_pred) - y_true))\n",
    "\n",
    "window_size = 25\n",
    "reshaped_train_x = np.reshape(train_x, train_x.shape + (1,))\n",
    "model = Sequential()\n",
    "model.add(LSTM(15, batch_input_shape=(2075, window_size, 1), dropout=0.5, recurrent_dropout=0.5, stateful=True)) # time step of 25, and input_dim of 1 per timestep\n",
    "model.add(Dense(2,activation=\"linear\"))\n",
    "model.add(Lambda(lambda x: x ** 2))\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStoppingByLossVal(monitor='loss', value=17, verbose=1),\n",
    "]\n",
    "optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=[accuracy])\n",
    "model.fit(reshaped_train_x, train_y, batch_size = 2075, verbose=2, epochs=200, shuffle=True)#, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "In a stateful network, you should only pass inputs with a number of samples that can be divided by the batch size. Found: 2075 samples. Batch size: 32.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-210-882ba381cd3f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mreshaped_test_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_along_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray_split\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices_or_sections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpredicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreshaped_test_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_columns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Date_to_use_on'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Date_to_use_on'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Date_to_use_on'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my_columns\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1153\u001b[0m                                  \u001b[1;34m'divided by the batch size. Found: '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m                                  \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' samples. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1155\u001b[1;33m                                  'Batch size: ' + str(batch_size) + '.')\n\u001b[0m\u001b[0;32m   1156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1157\u001b[0m         \u001b[1;31m# Prepare inputs, delegate logic to `predict_loop`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: In a stateful network, you should only pass inputs with a number of samples that can be divided by the batch size. Found: 2075 samples. Batch size: 32."
     ]
    }
   ],
   "source": [
    "reshaped_test_x = np.apply_along_axis(np.array_split, 1, test_x, indices_or_sections=window_size)\n",
    "predicted = np.round(model.predict(reshaped_test_x))\n",
    "result = pd.DataFrame(predicted, columns = y_columns)\n",
    "result['Date_to_use_on'] = test_data['Date_to_use_on']\n",
    "result = result[['Date_to_use_on'] + y_columns]\n",
    "result.to_csv(training_data_dir+ stock_symbol + '_test_parameters.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2075, 25, 1)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped_test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss= 264257929216.0000, Training Error= 489453.375\n",
      "Epoch 1, Loss= 479811232.0000, Training Error= 17106.129\n",
      "Epoch 100, Loss= 60911.2891, Training Error= 137.407\n",
      "Epoch 200, Loss= 33908.1562, Training Error= 89.705\n",
      "Epoch 300, Loss= 25045.1348, Training Error= 77.293\n",
      "Epoch 400, Loss= 19477.2129, Training Error= 68.778\n",
      "Epoch 500, Loss= 15974.9629, Training Error= 62.733\n",
      "Epoch 600, Loss= 13465.0459, Training Error= 57.997\n",
      "Epoch 700, Loss= 11610.8750, Training Error= 53.581\n",
      "Epoch 800, Loss= 10056.4551, Training Error= 50.333\n",
      "Epoch 900, Loss= 8817.3789, Training Error= 47.341\n",
      "Epoch 1000, Loss= 7787.1768, Training Error= 44.355\n",
      "Epoch 1100, Loss= 7028.2617, Training Error= 41.860\n",
      "Epoch 1200, Loss= 6302.0996, Training Error= 39.665\n",
      "Epoch 1300, Loss= 5750.4761, Training Error= 37.472\n",
      "Epoch 1400, Loss= 5276.0107, Training Error= 35.894\n",
      "Epoch 1500, Loss= 4782.9985, Training Error= 34.129\n",
      "Epoch 1600, Loss= 4413.1641, Training Error= 33.068\n",
      "Epoch 1700, Loss= 4043.2322, Training Error= 31.486\n",
      "Epoch 1800, Loss= 3743.1943, Training Error= 30.158\n",
      "Epoch 1900, Loss= 3452.7373, Training Error= 29.309\n",
      "Epoch 2000, Loss= 3200.0757, Training Error= 27.848\n",
      "Epoch 2100, Loss= 2960.8311, Training Error= 26.914\n",
      "Epoch 2200, Loss= 2789.9324, Training Error= 25.940\n",
      "Epoch 2300, Loss= 2600.9932, Training Error= 24.992\n",
      "Epoch 2400, Loss= 2440.5413, Training Error= 24.096\n",
      "Epoch 2500, Loss= 2298.7656, Training Error= 23.440\n",
      "Epoch 2600, Loss= 2148.7786, Training Error= 22.688\n",
      "Epoch 2700, Loss= 2024.5906, Training Error= 21.985\n",
      "Epoch 2800, Loss= 1896.5270, Training Error= 21.211\n",
      "Epoch 2900, Loss= 1800.7313, Training Error= 20.681\n",
      "Epoch 3000, Loss= 1695.9919, Training Error= 19.979\n",
      "Epoch 3100, Loss= 1609.8125, Training Error= 19.408\n",
      "Epoch 3200, Loss= 1522.9956, Training Error= 18.867\n",
      "Epoch 3300, Loss= 1433.6649, Training Error= 18.377\n",
      "Epoch 3400, Loss= 1340.5051, Training Error= 17.767\n",
      "Epoch 3500, Loss= 1306.9976, Training Error= 17.262\n",
      "Epoch 3600, Loss= 1221.6154, Training Error= 16.812\n",
      "Epoch 3700, Loss= 1168.9437, Training Error= 16.403\n",
      "Epoch 3800, Loss= 1098.6672, Training Error= 15.847\n",
      "Epoch 3900, Loss= 1040.6821, Training Error= 15.391\n",
      "Epoch 4000, Loss= 1004.0573, Training Error= 15.077\n",
      "Epoch 4100, Loss= 953.1461, Training Error= 14.585\n",
      "Epoch 4200, Loss= 951.3618, Training Error= 14.494\n",
      "Epoch 4300, Loss= 896.4062, Training Error= 14.117\n",
      "Epoch 4400, Loss= 835.9715, Training Error= 13.488\n",
      "Epoch 4500, Loss= 802.8412, Training Error= 13.104\n",
      "Epoch 4600, Loss= 765.9777, Training Error= 12.833\n",
      "Epoch 4700, Loss= 730.0213, Training Error= 12.432\n",
      "Epoch 4800, Loss= 700.7585, Training Error= 12.182\n",
      "Epoch 4900, Loss= 657.7686, Training Error= 11.799\n",
      "Epoch 5000, Loss= 622.5568, Training Error= 11.375\n",
      "Epoch 5100, Loss= 616.2896, Training Error= 11.345\n",
      "Epoch 5200, Loss= 559.7112, Training Error= 10.802\n",
      "Epoch 5300, Loss= 543.1952, Training Error= 10.534\n",
      "Epoch 5400, Loss= 521.5869, Training Error= 10.256\n",
      "Epoch 5500, Loss= 507.9224, Training Error= 10.042\n",
      "Epoch 5600, Loss= 488.5296, Training Error= 9.870\n",
      "Epoch 5700, Loss= 455.5491, Training Error= 9.508\n",
      "Epoch 5800, Loss= 437.3304, Training Error= 9.300\n",
      "Epoch 5900, Loss= 423.1651, Training Error= 9.182\n",
      "Epoch 6000, Loss= 410.3258, Training Error= 8.882\n",
      "Epoch 6100, Loss= 382.2233, Training Error= 8.639\n",
      "Epoch 6200, Loss= 372.0730, Training Error= 8.430\n",
      "Epoch 6300, Loss= 352.0682, Training Error= 8.149\n",
      "Epoch 6400, Loss= 339.5847, Training Error= 7.911\n",
      "Epoch 6500, Loss= 333.6566, Training Error= 7.860\n",
      "Epoch 6600, Loss= 310.8376, Training Error= 7.497\n",
      "Epoch 6700, Loss= 301.4578, Training Error= 7.417\n",
      "Epoch 6800, Loss= 288.7577, Training Error= 7.196\n",
      "Epoch 6900, Loss= 277.3246, Training Error= 6.986\n",
      "Epoch 7000, Loss= 264.8981, Training Error= 6.802\n",
      "Epoch 7100, Loss= 256.8147, Training Error= 6.645\n",
      "Epoch 7200, Loss= 269.2217, Training Error= 6.958\n",
      "Epoch 7300, Loss= 243.7814, Training Error= 6.390\n",
      "Epoch 7400, Loss= 249.7665, Training Error= 6.593\n",
      "Epoch 7500, Loss= 229.1315, Training Error= 6.192\n",
      "Epoch 7600, Loss= 216.2764, Training Error= 5.901\n",
      "Epoch 7700, Loss= 208.8183, Training Error= 5.775\n",
      "Epoch 7800, Loss= 202.3318, Training Error= 5.670\n",
      "Epoch 7900, Loss= 194.4885, Training Error= 5.490\n",
      "Epoch 8000, Loss= 190.2792, Training Error= 5.398\n",
      "Epoch 8100, Loss= 183.1347, Training Error= 5.278\n",
      "Epoch 8200, Loss= 182.9143, Training Error= 5.297\n",
      "Epoch 8300, Loss= 177.2091, Training Error= 5.217\n",
      "Epoch 8400, Loss= 165.2505, Training Error= 4.896\n",
      "Epoch 8500, Loss= 162.7934, Training Error= 4.835\n",
      "Epoch 8600, Loss= 159.9075, Training Error= 4.731\n",
      "Epoch 8700, Loss= 163.2581, Training Error= 4.817\n",
      "Epoch 8800, Loss= 152.4846, Training Error= 4.593\n",
      "Epoch 8900, Loss= 146.6055, Training Error= 4.475\n",
      "Epoch 9000, Loss= 142.4542, Training Error= 4.353\n",
      "Epoch 9100, Loss= 141.9214, Training Error= 4.365\n",
      "Epoch 9200, Loss= 135.7928, Training Error= 4.184\n",
      "Epoch 9300, Loss= 131.6416, Training Error= 4.071\n",
      "Epoch 9400, Loss= 129.2528, Training Error= 4.000\n",
      "Epoch 9500, Loss= 127.7727, Training Error= 3.966\n",
      "Epoch 9600, Loss= 120.3494, Training Error= 3.821\n",
      "Epoch 9700, Loss= 121.8952, Training Error= 3.853\n",
      "Epoch 9800, Loss= 116.2301, Training Error= 3.672\n",
      "Epoch 9900, Loss= 113.6431, Training Error= 3.603\n",
      "Epoch 10000, Loss= 115.3101, Training Error= 3.657\n",
      "Epoch 10100, Loss= 108.1295, Training Error= 3.453\n",
      "Epoch 10200, Loss= 104.2981, Training Error= 3.339\n",
      "Epoch 10300, Loss= 102.6820, Training Error= 3.288\n",
      "Epoch 10400, Loss= 102.0057, Training Error= 3.328\n",
      "Epoch 10500, Loss= 100.7186, Training Error= 3.137\n",
      "Epoch 10600, Loss= 96.9678, Training Error= 3.093\n",
      "Epoch 10700, Loss= 106.1879, Training Error= 3.495\n",
      "Epoch 10800, Loss= 94.3429, Training Error= 2.985\n",
      "Epoch 10900, Loss= 91.1714, Training Error= 2.898\n",
      "Epoch 11000, Loss= 90.5547, Training Error= 2.840\n",
      "Epoch 11100, Loss= 86.3754, Training Error= 2.713\n",
      "Epoch 11200, Loss= 86.9229, Training Error= 2.661\n",
      "Epoch 11300, Loss= 83.2378, Training Error= 2.599\n",
      "Epoch 11400, Loss= 88.9203, Training Error= 2.839\n",
      "Epoch 11500, Loss= 79.3204, Training Error= 2.459\n",
      "Epoch 11600, Loss= 78.4052, Training Error= 2.392\n",
      "Epoch 11700, Loss= 75.5277, Training Error= 2.317\n",
      "Epoch 11800, Loss= 80.8585, Training Error= 2.610\n",
      "Epoch 11900, Loss= 72.4544, Training Error= 2.195\n",
      "Epoch 12000, Loss= 72.1106, Training Error= 2.164\n",
      "Epoch 12100, Loss= 69.4854, Training Error= 2.106\n",
      "Epoch 12200, Loss= 66.5216, Training Error= 1.988\n",
      "Epoch 12300, Loss= 68.0159, Training Error= 1.980\n",
      "Epoch 12400, Loss= 63.5769, Training Error= 1.850\n",
      "Epoch 12500, Loss= 60.4381, Training Error= 1.745\n",
      "Epoch 12600, Loss= 59.5983, Training Error= 1.680\n",
      "Epoch 12700, Loss= 60.0514, Training Error= 1.697\n",
      "Epoch 12800, Loss= 58.3717, Training Error= 1.621\n",
      "Epoch 12900, Loss= 56.2467, Training Error= 1.525\n",
      "Epoch 13000, Loss= 58.9486, Training Error= 1.662\n",
      "Epoch 13100, Loss= 55.5243, Training Error= 1.511\n",
      "Epoch 13200, Loss= 52.3547, Training Error= 1.326\n",
      "Epoch 13300, Loss= 51.1278, Training Error= 1.277\n",
      "Epoch 13400, Loss= 48.7463, Training Error= 1.213\n",
      "Epoch 13500, Loss= 47.9198, Training Error= 1.175\n",
      "Epoch 13600, Loss= 46.5073, Training Error= 1.119\n",
      "Epoch 13700, Loss= 46.9796, Training Error= 1.095\n",
      "Epoch 13800, Loss= 44.4647, Training Error= 1.007\n",
      "Epoch 13900, Loss= 43.2206, Training Error= 0.946\n",
      "Epoch 14000, Loss= 42.2984, Training Error= 0.892\n",
      "Epoch 14100, Loss= 12312.4004, Training Error= 70.656\n",
      "Epoch 14200, Loss= 281.4556, Training Error= 4.548\n",
      "Epoch 14300, Loss= 197.6125, Training Error= 3.223\n",
      "Epoch 14400, Loss= 153.3562, Training Error= 2.714\n",
      "Epoch 14500, Loss= 135.2698, Training Error= 2.258\n",
      "Epoch 14600, Loss= 119.7762, Training Error= 1.903\n",
      "Epoch 14700, Loss= 96.6472, Training Error= 1.553\n",
      "Epoch 14800, Loss= 85.5135, Training Error= 1.295\n",
      "Epoch 14900, Loss= 73.0504, Training Error= 1.012\n",
      "Epoch 15000, Loss= 70.0104, Training Error= 0.859\n",
      "Epoch 15100, Loss= 70.3456, Training Error= 0.645\n",
      "Epoch 15200, Loss= 63.2858, Training Error= 0.544\n",
      "Epoch 15300, Loss= 59.6055, Training Error= 0.433\n",
      "Epoch 15400, Loss= 59.2809, Training Error= 0.313\n",
      "Epoch 15500, Loss= 55.2743, Training Error= 0.181\n",
      "Epoch 15600, Loss= 50.0779, Training Error= 0.147\n",
      "Epoch 15700, Loss= 47.8984, Training Error= 0.050\n",
      "Epoch 15800, Loss= 46.1417, Training Error= 0.050\n",
      "Epoch 15900, Loss= 42.9210, Training Error= 0.091\n",
      "Epoch 16000, Loss= 42.5503, Training Error= 0.188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16100, Loss= 42.2734, Training Error= 0.207\n",
      "Epoch 16200, Loss= 37.9294, Training Error= 0.250\n",
      "Epoch 16300, Loss= 37.2428, Training Error= 0.334\n",
      "Epoch 16400, Loss= 33.6662, Training Error= 0.355\n",
      "Epoch 16500, Loss= 34.9882, Training Error= 0.384\n",
      "Epoch 16600, Loss= 31.3605, Training Error= 0.475\n",
      "Epoch 16700, Loss= 28.9219, Training Error= 0.520\n",
      "Epoch 16800, Loss= 28.5135, Training Error= 0.555\n",
      "Epoch 16900, Loss= 27.5463, Training Error= 0.591\n",
      "Epoch 17000, Loss= 27.0188, Training Error= 0.634\n",
      "Epoch 17100, Loss= 27.1599, Training Error= 0.627\n",
      "Epoch 17200, Loss= 26.0712, Training Error= 0.697\n",
      "Epoch 17300, Loss= 25.9538, Training Error= 0.727\n",
      "Epoch 17400, Loss= 25.3529, Training Error= 0.734\n",
      "Epoch 17500, Loss= 24.8879, Training Error= 0.769\n",
      "Epoch 17600, Loss= 24.8598, Training Error= 0.710\n",
      "Epoch 17700, Loss= 24.3800, Training Error= 0.786\n",
      "Epoch 17800, Loss= 24.3019, Training Error= 0.779\n",
      "Epoch 17900, Loss= 23.4602, Training Error= 0.841\n",
      "Epoch 18000, Loss= 23.2424, Training Error= 0.840\n",
      "Epoch 18100, Loss= 22.5603, Training Error= 0.835\n",
      "Epoch 18200, Loss= 22.6539, Training Error= 0.820\n",
      "Epoch 18300, Loss= 22.5402, Training Error= 0.834\n",
      "Epoch 18400, Loss= 21.8364, Training Error= 0.874\n",
      "Epoch 18500, Loss= 21.5169, Training Error= 0.897\n",
      "Epoch 18600, Loss= 21.3562, Training Error= 0.914\n",
      "Epoch 18700, Loss= 21.0154, Training Error= 0.924\n",
      "Epoch 18800, Loss= 20.8004, Training Error= 0.927\n",
      "Epoch 18900, Loss= 20.6504, Training Error= 0.967\n",
      "Epoch 19000, Loss= 20.5274, Training Error= 0.929\n",
      "Epoch 19100, Loss= 20.4268, Training Error= 0.936\n",
      "Epoch 19200, Loss= 20.2395, Training Error= 0.976\n",
      "Epoch 19300, Loss= 20.2331, Training Error= 1.027\n",
      "Epoch 19400, Loss= 19.8773, Training Error= 0.912\n",
      "[[7.9017846e-05 5.6982338e-01]\n",
      " [1.7035719e+00 1.8648262e+00]\n",
      " [2.4268374e-01 4.9295530e+00]\n",
      " ...\n",
      " [5.6413999e+00 1.3469201e+01]\n",
      " [2.0783851e+00 1.0153975e+01]\n",
      " [3.7027247e-02 2.9147668e+00]]\n",
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "'''if train_model:\n",
    "\n",
    "    # Initialize the variables (i.e. assign their default value)\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # Run initializer\n",
    "        sess.run(init)\n",
    "        for epoch in range(num_epochs):\n",
    "            \n",
    "            shuffled_indexes = np.random.permutation(len(train_x))\n",
    "    \n",
    "            \n",
    "            for i in shuffled_indexes[::batch_size]:\n",
    "                batch_x = train_x[i:i+batch_size]\n",
    "                batch_y = train_y[i:i+batch_size]\n",
    "\n",
    "                sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "\n",
    "            if epoch % display_epoch == 0 or epoch == 1:\n",
    "                loss, acc = sess.run([loss_op, accuracy], feed_dict={X: train_x, Y: train_y})\n",
    "\n",
    "                print(\"Epoch \" + str(epoch) + \", Loss= \" + \\\n",
    "                      \"{:.4f}\".format(loss) + \", Training Error= \" + \\\n",
    "                      \"{:.3f}\".format(acc))\n",
    "\n",
    "            if loss<20:\n",
    "                break\n",
    "        print(sess.run(output, feed_dict={X: train_x}))\n",
    "        saver.save(sess, model_dir + stock_symbol + '_regularizer.ckpt')\n",
    "        \n",
    "        \n",
    "        print('Model Saved')\n",
    "\n",
    "      '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./regularizer_models/c/c_regularizer.ckpt\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Key Adam/beta_1 not found in checkpoint\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n\nCaused by op 'save/RestoreV2', defined at:\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\asyncio\\base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\asyncio\\base_events.py\", line 1434, in _run_once\n    handle._run()\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 122, in _handle_events\n    handler_func(fileobj, events)\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tornado\\stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tornado\\stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-5-035695279ded>\", line 3, in <module>\n    saver = tf.train.Saver()\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1284, in __init__\n    self.build()\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1296, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1333, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 781, in _build_internal\n    restore_sequentially, reshape)\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 400, in _AddRestoreOps\n    restore_sequentially)\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 832, in bulk_restore\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 1546, in restore_v2\n    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3414, in create_op\n    op_def=op_def)\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1740, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nNotFoundError (see above for traceback): Key Adam/beta_1 not found in checkpoint\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Key Adam/beta_1 not found in checkpoint\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-035695279ded>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m         var_to_shape_map = reader.get_variable_to_shape_map()'''\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_dir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstock_symbol\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_regularizer.ckpt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Model restored.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mpredicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36mrestore\u001b[1;34m(self, sess, save_path)\u001b[0m\n\u001b[0;32m   1766\u001b[0m         \u001b[0mshould_reraise\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1767\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mshould_reraise\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1768\u001b[1;33m         \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexception_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexception_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexception_traceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1769\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mexception_traceback\u001b[0m  \u001b[1;31m# avoid reference cycles\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1770\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m    691\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    692\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 693\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    694\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36mrestore\u001b[1;34m(self, sess, save_path)\u001b[0m\n\u001b[0;32m   1750\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1751\u001b[0m         sess.run(self.saver_def.restore_op_name,\n\u001b[1;32m-> 1752\u001b[1;33m                  {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[0;32m   1753\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNotFoundError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1754\u001b[0m       \u001b[0mexception_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexception_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexception_traceback\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1333\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1334\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1335\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1337\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Key Adam/beta_1 not found in checkpoint\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n\nCaused by op 'save/RestoreV2', defined at:\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\asyncio\\base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\asyncio\\base_events.py\", line 1434, in _run_once\n    handle._run()\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 122, in _handle_events\n    handler_func(fileobj, events)\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tornado\\stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tornado\\stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-5-035695279ded>\", line 3, in <module>\n    saver = tf.train.Saver()\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1284, in __init__\n    self.build()\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1296, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1333, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 781, in _build_internal\n    restore_sequentially, reshape)\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 400, in _AddRestoreOps\n    restore_sequentially)\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 832, in bulk_restore\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 1546, in restore_v2\n    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3414, in create_op\n    op_def=op_def)\n  File \"c:\\users\\jing yang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1740, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nNotFoundError (see above for traceback): Key Adam/beta_1 not found in checkpoint\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n"
     ]
    }
   ],
   "source": [
    "if test_model:\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        '''var_name_list = [v.name for v in tf.trainable_variables()]\n",
    "        print(var_name_list)\n",
    "        reader = pywrap_tensorflow.NewCheckpointReader(model_dir + stock_symbol + '_regularizer.ckpt')\n",
    "        var_to_shape_map = reader.get_variable_to_shape_map()'''\n",
    "        \n",
    "        saver.restore(sess, model_dir + stock_symbol + '_regularizer.ckpt')\n",
    "        print(\"Model restored.\")\n",
    "        predicted = tf.round(sess.run(output, feed_dict={X : test_x})).eval()\n",
    "        predicted = np.array(predicted)\n",
    "        \n",
    "    result = pd.DataFrame(predicted, columns = y_columns)\n",
    "    result['Date_to_use_on'] = test_data['Date_to_use_on']\n",
    "    result = result[['Date_to_use_on'] + y_columns]\n",
    "    result.to_csv(training_data_dir+ stock_symbol + '_test_parameters.csv', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date_to_use_on</th>\n",
       "      <th>ppo</th>\n",
       "      <th>rsi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-11-20</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-11-23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-11-24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-11-25</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-11-27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2009-11-30</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2009-12-01</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2009-12-02</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2009-12-03</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2009-12-04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2009-12-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2009-12-08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2009-12-09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2009-12-10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2009-12-11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2009-12-14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2009-12-15</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2009-12-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2009-12-17</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2009-12-18</td>\n",
       "      <td>38.0</td>\n",
       "      <td>2366.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2009-12-21</td>\n",
       "      <td>3.0</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2009-12-22</td>\n",
       "      <td>93.0</td>\n",
       "      <td>227.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2009-12-23</td>\n",
       "      <td>224.0</td>\n",
       "      <td>937.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2009-12-24</td>\n",
       "      <td>3.0</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2009-12-28</td>\n",
       "      <td>5.0</td>\n",
       "      <td>141.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2009-12-29</td>\n",
       "      <td>3.0</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2009-12-30</td>\n",
       "      <td>5.0</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2009-12-31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3135.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2010-01-05</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3951.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>2012-06-14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>2012-06-15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>2012-06-18</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648</th>\n",
       "      <td>2012-06-19</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>2012-06-20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650</th>\n",
       "      <td>2012-06-21</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651</th>\n",
       "      <td>2012-06-22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652</th>\n",
       "      <td>2012-06-25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653</th>\n",
       "      <td>2012-06-26</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>654</th>\n",
       "      <td>2012-06-27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>655</th>\n",
       "      <td>2012-06-28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656</th>\n",
       "      <td>2012-06-29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>657</th>\n",
       "      <td>2012-07-02</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658</th>\n",
       "      <td>2012-07-03</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>659</th>\n",
       "      <td>2012-07-05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>2012-07-06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661</th>\n",
       "      <td>2012-07-09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>2012-07-10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>663</th>\n",
       "      <td>2012-07-11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664</th>\n",
       "      <td>2012-07-12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>2012-07-13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>666</th>\n",
       "      <td>2012-07-16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>667</th>\n",
       "      <td>2012-07-17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668</th>\n",
       "      <td>2012-07-18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669</th>\n",
       "      <td>2012-07-19</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>2012-07-20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>2012-07-23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>2012-07-24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>2012-07-25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>2012-07-26</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>675 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Date_to_use_on    ppo     rsi\n",
       "0       2009-11-20    4.0     1.0\n",
       "1       2009-11-23    0.0     4.0\n",
       "2       2009-11-24    0.0     1.0\n",
       "3       2009-11-25    2.0     0.0\n",
       "4       2009-11-27    0.0     0.0\n",
       "5       2009-11-30    6.0     2.0\n",
       "6       2009-12-01    2.0     0.0\n",
       "7       2009-12-02    1.0     0.0\n",
       "8       2009-12-03    1.0     4.0\n",
       "9       2009-12-04    0.0    11.0\n",
       "10      2009-12-07    0.0     9.0\n",
       "11      2009-12-08    0.0     7.0\n",
       "12      2009-12-09    0.0     1.0\n",
       "13      2009-12-10    0.0     0.0\n",
       "14      2009-12-11    0.0     0.0\n",
       "15      2009-12-14    0.0    10.0\n",
       "16      2009-12-15    2.0     4.0\n",
       "17      2009-12-16    1.0     2.0\n",
       "18      2009-12-17    1.0     0.0\n",
       "19      2009-12-18   38.0  2366.0\n",
       "20      2009-12-21    3.0    38.0\n",
       "21      2009-12-22   93.0   227.0\n",
       "22      2009-12-23  224.0   937.0\n",
       "23      2009-12-24    3.0    33.0\n",
       "24      2009-12-28    5.0   141.0\n",
       "25      2009-12-29    3.0    39.0\n",
       "26      2009-12-30    5.0    32.0\n",
       "27      2009-12-31    0.0     3.0\n",
       "28      2010-01-04    1.0  3135.0\n",
       "29      2010-01-05   11.0  3951.0\n",
       "..             ...    ...     ...\n",
       "645     2012-06-14    1.0     2.0\n",
       "646     2012-06-15    0.0     3.0\n",
       "647     2012-06-18    1.0     3.0\n",
       "648     2012-06-19    1.0     1.0\n",
       "649     2012-06-20    0.0     0.0\n",
       "650     2012-06-21    1.0     2.0\n",
       "651     2012-06-22    0.0     1.0\n",
       "652     2012-06-25    0.0     1.0\n",
       "653     2012-06-26    1.0     5.0\n",
       "654     2012-06-27    0.0     1.0\n",
       "655     2012-06-28    0.0     2.0\n",
       "656     2012-06-29    0.0     3.0\n",
       "657     2012-07-02    2.0     5.0\n",
       "658     2012-07-03    1.0     0.0\n",
       "659     2012-07-05    0.0     0.0\n",
       "660     2012-07-06    0.0     0.0\n",
       "661     2012-07-09    0.0     1.0\n",
       "662     2012-07-10    1.0     1.0\n",
       "663     2012-07-11    1.0     0.0\n",
       "664     2012-07-12    0.0     1.0\n",
       "665     2012-07-13    0.0     0.0\n",
       "666     2012-07-16    0.0     2.0\n",
       "667     2012-07-17    0.0     1.0\n",
       "668     2012-07-18    0.0     2.0\n",
       "669     2012-07-19    1.0     1.0\n",
       "670     2012-07-20    0.0     2.0\n",
       "671     2012-07-23    0.0     1.0\n",
       "672     2012-07-24    0.0     2.0\n",
       "673     2012-07-25    0.0     2.0\n",
       "674     2012-07-26    1.0     3.0\n",
       "\n",
       "[675 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
